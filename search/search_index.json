{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"Welcome to AccuKnox Help What is AccuKnox \u00b6 AccuKnox is Identity-driven Security for Data, Kubernetes, and Native VM workloads for Private and Public Clouds. AccuKnox is Zero Trust Run-time Kubernetes Security Solution. It provides run-time protection for your Kubernetes and other cloud workloads using Kernel Native Primitives such as AppArmor SELinux and eBPF. Quick Start \u00b6 Read more \ud83d\udc49 Documentation - Getting Started Links \u00b6 Cilium KubeArmor","title":"Home"},{"location":"#what_is_accuknox","text":"AccuKnox is Identity-driven Security for Data, Kubernetes, and Native VM workloads for Private and Public Clouds. AccuKnox is Zero Trust Run-time Kubernetes Security Solution. It provides run-time protection for your Kubernetes and other cloud workloads using Kernel Native Primitives such as AppArmor SELinux and eBPF.","title":"What is AccuKnox"},{"location":"#quick_start","text":"Read more \ud83d\udc49 Documentation - Getting Started","title":"Quick Start"},{"location":"#links","text":"Cilium KubeArmor","title":"Links"},{"location":"license/","text":"License \u00b6 MIT License The graduate cap icon made by Freepik from www.flaticon.com is licensed by CC 3.0 BY","title":"License"},{"location":"license/#license","text":"MIT License The graduate cap icon made by Freepik from www.flaticon.com is licensed by CC 3.0 BY","title":"License"},{"location":"accuknox-onprem/FAQ/","text":"FAQ \u00b6 1. Can I skip Pinot and send logs to my elastic or splunk cluster? \u00b6 Yes, it's feasible. The feeder agent sends the logs to /var/log/*.log which can be pushed to ELK stack. Please refer accuknox-onprem/elastic for specific details. 2.Can I send metrics to my time series? \u00b6 Yes, it's feasible.Install a GRPc server, with TCP IP and port. Map the GRPC server IP and port in Feeder Service. method","title":"FAQ"},{"location":"accuknox-onprem/FAQ/#faq","text":"","title":"FAQ"},{"location":"accuknox-onprem/FAQ/#1_can_i_skip_pinot_and_send_logs_to_my_elastic_or_splunk_cluster","text":"Yes, it's feasible. The feeder agent sends the logs to /var/log/*.log which can be pushed to ELK stack. Please refer accuknox-onprem/elastic for specific details.","title":"1. Can I skip Pinot and send logs to my elastic or splunk cluster?"},{"location":"accuknox-onprem/FAQ/#2can_i_send_metrics_to_my_time_series","text":"Yes, it's feasible.Install a GRPc server, with TCP IP and port. Map the GRPC server IP and port in Feeder Service. method","title":"2.Can I send metrics to my time series?"},{"location":"accuknox-onprem/agents-install/","text":"Installing Helm \u00b6 This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases. From the Binary Releases \u00b6 Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install Agents helm package: \u00b6 helm repo add accuknox-onprem-agents https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-agents helm repo update helm search repo accuknox-onprem-agents Follow the below order to install agents on k8s cluster. Cilium \u00b6 Installation using Helm Step 1: helm repo add cilium https://helm.cilium.io/ Step 2: helm install cilium cilium/cilium --version 1 .10.5 --namespace kube-system FYR: https://docs.cilium.io/en/v1.10/gettingstarted/k8s-install-helm/ kArmor \u00b6 kArmor is a CLI client to help manage KubeArmor. KubeArmor is a container-aware runtime security enforcement system that restricts the behavior (such as process execution, file access, and networking operation) of containers at the system level. Installation \u00b6 curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sh To build and install, clone the repository and make install \u00b6 FYR: https://github.com/kubearmor/kubearmor-client Shared-informer-agent \u00b6 Step 1 : Create namespace has accuknox-dev-shared-informer-agent Eg: kubectl create ns accuknox-dev-shared-informer-agent Step 2 : Install using helm helm upgrade --install shared-informer-agent-chart-1.0.2.tgz -n accuknox-dev-shared-informer-agent S3-audit-reporter \u00b6 Step 1 : Create namespace has accuknox-dev-s3-audit-reporter Eg. kubectl create ns accuknox-dev-s3-audit-reporter Step 2 : Install using helm Eg. helm upgrade --install s3-audit-reporter-charts-1.0.1.tgz -n accuknox-dev-s3-audit-reporter Feeder-Service \u00b6 Step 1 : Create namespace has feeder-service Eg. kubectl create ns feeder-service Step 2 : Install using helm Eg. helm upgrade --install feeder-service-0.1.0.tgz -n feeder-service Knox-Containersec \u00b6 Step 1 : Create namespace has accuknox-onprem-agents kubectl create ns accuknox-agents Step 2 : Install using helm Eg. helm upgrade --install knox-containersec-chart-0.1.0.tgz -n accuknox-agents Policy Enforcement Agent \u00b6 Install using helm helm upgrade --install policy-enforcement-agent-1.0.2.tgz -n accuknox-agents","title":"How to install?"},{"location":"accuknox-onprem/agents-install/#installing_helm","text":"This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.","title":"Installing Helm"},{"location":"accuknox-onprem/agents-install/#from_the_binary_releases","text":"Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here..","title":"From the Binary Releases"},{"location":"accuknox-onprem/agents-install/#add_accuknox_repository_to_install_agents_helm_package","text":"helm repo add accuknox-onprem-agents https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-agents helm repo update helm search repo accuknox-onprem-agents Follow the below order to install agents on k8s cluster.","title":"Add accuknox repository to install Agents helm package:"},{"location":"accuknox-onprem/agents-install/#cilium","text":"Installation using Helm Step 1: helm repo add cilium https://helm.cilium.io/ Step 2: helm install cilium cilium/cilium --version 1 .10.5 --namespace kube-system FYR: https://docs.cilium.io/en/v1.10/gettingstarted/k8s-install-helm/","title":"Cilium"},{"location":"accuknox-onprem/agents-install/#karmor","text":"kArmor is a CLI client to help manage KubeArmor. KubeArmor is a container-aware runtime security enforcement system that restricts the behavior (such as process execution, file access, and networking operation) of containers at the system level.","title":"kArmor"},{"location":"accuknox-onprem/agents-install/#installation","text":"curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sh","title":"Installation"},{"location":"accuknox-onprem/agents-install/#to_build_and_install_clone_the_repository_and_make_install","text":"FYR: https://github.com/kubearmor/kubearmor-client","title":"To build and install, clone the repository and make install"},{"location":"accuknox-onprem/agents-install/#shared-informer-agent","text":"Step 1 : Create namespace has accuknox-dev-shared-informer-agent Eg: kubectl create ns accuknox-dev-shared-informer-agent Step 2 : Install using helm helm upgrade --install shared-informer-agent-chart-1.0.2.tgz -n accuknox-dev-shared-informer-agent","title":"Shared-informer-agent"},{"location":"accuknox-onprem/agents-install/#s3-audit-reporter","text":"Step 1 : Create namespace has accuknox-dev-s3-audit-reporter Eg. kubectl create ns accuknox-dev-s3-audit-reporter Step 2 : Install using helm Eg. helm upgrade --install s3-audit-reporter-charts-1.0.1.tgz -n accuknox-dev-s3-audit-reporter","title":"S3-audit-reporter"},{"location":"accuknox-onprem/agents-install/#feeder-service","text":"Step 1 : Create namespace has feeder-service Eg. kubectl create ns feeder-service Step 2 : Install using helm Eg. helm upgrade --install feeder-service-0.1.0.tgz -n feeder-service","title":"Feeder-Service"},{"location":"accuknox-onprem/agents-install/#knox-containersec","text":"Step 1 : Create namespace has accuknox-onprem-agents kubectl create ns accuknox-agents Step 2 : Install using helm Eg. helm upgrade --install knox-containersec-chart-0.1.0.tgz -n accuknox-agents","title":"Knox-Containersec"},{"location":"accuknox-onprem/agents-install/#policy_enforcement_agent","text":"Install using helm helm upgrade --install policy-enforcement-agent-1.0.2.tgz -n accuknox-agents","title":"Policy Enforcement Agent"},{"location":"accuknox-onprem/agents-verify/","text":"","title":"How to verify?"},{"location":"accuknox-onprem/ak-architecture-diagram-purpose/","text":"","title":"Purpose"},{"location":"accuknox-onprem/ak-architecture-diagram/","text":"","title":"Diagram"},{"location":"accuknox-onprem/core-components-install/","text":"Installing Helm \u00b6 This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases. From the Binary Releases \u00b6 Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install Core Components helm package: \u00b6 helm repo add accuknox-onprem-core-components https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-core-components helm repo update helm search repo accuknox-onprem-core-components Follow the below order to install core components on k8s cluster User-management-service: \u00b6 Step 1 : Create namespace has accuknox-dev-user-mgmt Eg. kubectl create ns accuknox-dev-user-mgmt Step 2 : Install using helm Eg. helm upgrade --install user-management-service-0.1.0.tgz -n accuknox-dev-user-mgmt Agents-Auth-Service \u00b6 Step 1 : Create namespace has accuknox-dev-agents-auth-service Eg. kubectl create ns accuknox-dev-agents-auth-service Step 2 : Install using helm Eg. helm upgrade --install agents-auth-service-charts-0.1.0.tgz -n accuknox-dev-agents-auth-service Anomaly-detection-management \u00b6 Step 1 : Create namespace has accuknox-dev-ad-mgmt Eg. kubectl create ns accuknox-dev-ad-mgmt Step 2 : Install using helm Eg. helm upgrade --install anomaly-detection-mgmt-chart-0.1.0.tgz -n accuknox-dev-ad-mgmt Anomaly-detection-publisher-core \u00b6 Step 1 : Create namespace has accuknox-dev-ad-core Eg. kubectl create ns accuknox-dev-ad-core Step 2 : Install using helm Eg. helm upgrade --install anomaly-detection-publisher-core-chart-1.0.4.tgz -n accuknox-dev-ad-core Data-protection-mgmt \u00b6 Step 1 : Create namespace has accuknox-dev-dp-mgmt Eg. kubectl create ns accuknox-dev-dp-mgmt Step 2 : Install using helm Eg. helm upgrade --install data-protection-mgmt-0.1.0.tgz -n accuknox-dev-dp-mgmt Data-protection-core \u00b6 Step 1 : Create namespace has accuknox-dev-dp-core Eg. kubectl create ns accuknox-dev-dp-core Step 2 : Install using helm Eg. helm upgrade --install data-protection-core-1.0.4.tgz -n accuknox-dev-dp-core Data-protection-consumer \u00b6 Step 1 : Create namespace has accuknox-dev-dp-core (Since ns already this step is optional) Eg. kubectl create ns accuknox-dev-dp-core Step 2 : Install using helm Eg. helm upgrade --install data-protection-consumer-0.1.0.tgz -n accuknox-dev-dp-core S3-audit-report-consumer \u00b6 Step 1 : Create namespace has accuknox-dev-s3-audit-reporter-consumer Eg. kubectl create ns accuknox-dev-s3-audit-reporter-consumer Step 2 : Install using helm Eg. helm upgrade --install s3-audit-reporter-consumer-charts-0.1.0.tgz -n accuknox-dev-s3-audit-reporter-consumer Dp-db-audit-log-processor \u00b6 Step 1 : Create namespace has accuknox-dev-dp-core kubectl create ns accuknox-dev-dp-core Step 2 : Install using helm Eg. helm upgrade --install dp-db-audit-log-processor-chart-0.1.0.tgz -n accuknox-dev-dp-core Data-classification-pipeline-consumer \u00b6 Step 1 : Create namespace has accuknox-dev-data-classification-pipeline-consumer Eg. kubectl create ns accuknox-dev-data-classification-pipeline-consumer Step 2 : Install using helm Eg. helm upgrade --install data-classification-pipeline-consumer-chart-0.1.0.tgz -n accuknox-dev-data-classification-pipeline-consumer Cluster-management-service \u00b6 Step 1 : Create namespace has accuknox-dev-cluster-mgmt Eg. kubectl create ns accuknox-dev-cluster-mgmt Step 2 : Install using helm Eg. helm upgrade --install cluster-management-service-chart-0.1.0.tgz -n accuknox-dev-cluster-mgmt Agent-data-collector \u00b6 Step 1 : Create namespace has accuknox-dev-adc Eg. kubectl create ns accuknox-dev-adc Step 2 : Install using helm Eg. helm upgrade --install agent-data-collector-charts-0.1.0.tgz -n accuknox-dev-adc Cluster-onboarding-service \u00b6 Step 1 : Create namespace has accuknox-dev-cluster-onboard Eg. kubectl create ns accuknox-dev-cluster-onboard Step 2 : Install using helm Eg. helm upgrade --install cluster-onboarding-service-0.1.0.tgz -n accuknox-dev-cluster-onboard Cluster-entity-daemon \u00b6 Step 1 : Create namespace has accuknox-dev-cluster-entity-daemon Eg: kubectl create ns accuknox-dev-cluster-entity-daemon Step 2 : Install using helm Eg. helm upgrade --install cluster-entity-daemon-chart-0.1.0.tgz -n accuknox-dev-cluster-entity-daemon Shared-informer-service \u00b6 Step 1 : Create namespace has accuknox-dev-shared-informer-service Eg. kubectl create ns accuknox-dev-shared-informer-service Step 2 : Install using helm Eg. helm upgrade --install shared-informer-service-chart-0.1.0.tgz -n accuknox-dev-shared-informer-service Data-pipeline-api \u00b6 Step 1 : Create namespace has accuknox-dev-datapipeline-api Eg. kubectl create ns accuknox-dev-datapipeline-api Step 2 : Install using helm Eg. helm upgrade --install data-pipeline-api-charts-0.1.0.tgz -n accuknox-dev-datapipeline-api Datapipeline-temporal \u00b6 Step 1 : Create namespace has accuknox-dev-temporal Eg. kubectl create ns accuknox-dev-temporal Step 2 : Install using helm Eg. helm upgrade --install datapipeline-temporal-charts-0.1.0.tgz -n accuknox-dev-temporal Data-pipeline-samza-jobs \u00b6 Step 1 : Create namespace has accuknox-dev-samzajobs Eg. kubectl create ns accuknox-dev-samzajobs Step 2 : Install using helm Eg. helm upgrade --install datapipeline-samza-0.1.0.tgz -n accuknox-dev-samzajobs Feeder-grpc-server \u00b6 Step 1 : Create namespace has accuknox-dev-feeder-grpc-server Eg. kubectl create ns accuknox-dev-feeder-grpc-server Step 2 : Install using helm Eg. helm upgrade --install feeder-grpc-server-chart-0.1.0.tgz -n accuknox-dev-feeder-grpc-server Policy-service Step 1 : Create namespace has accuknox-dev-policy-service Eg. kubectl create ns accuknox-dev-policy-service Step 2 : Install using helm Eg. helm upgrade --install policy-service-charts-0.1.0.tgz -n accuknox-dev-policy-service Policy-daemon \u00b6 Step 1 : Create namespace has accuknox-dev-policy-daemon Eg. kubectl create ns accuknox-dev-policy-daemon Step 2 : Install using helm Eg. helm upgrade --install policy-daemon-charts-0.1.0.tgz -n accuknox-dev-policy-daemon Policy-provider-service \u00b6 Step 1 : Create namespace has accuknox-dev-policy-provider-service Eg. kubectl create ns accuknox-dev-policy-provider-service Step 2 : Install using helm Eg. helm upgrade --install policy-provider-service-0.1.0.tgz -n accuknox-dev-policy-provider-service Workload-identity-daemon \u00b6 Step 1 : Create namespace has accuknox-dev-workload-identity-daemon Eg. kubectl create ns accuknox-dev-workload-identity-daemon Step 2 : Install using helm Eg. helm upgrade --install workload-identity-daemon-chart-0.1.0.tgz -n accuknox-dev-workload-identity-daemon Recommended-policy-daemon \u00b6 Step 1 : Create namespace has accuknox-dev-recommended-policy-daemon Eg. kubectl create ns accuknox-dev-recommended-policy-daemon Step 2 : Install using helm Eg. helm upgrade --install recommended-policy-daemon-1.0.4.tgz -n accuknox-dev-recommended-policy-daemon Discoveredpolicy-daemon Step 1 : Create namespace has accuknox-dev-discovered-policy-daemon Eg. kubectl create ns accuknox-dev-discovered-policy-daemon Step 2 : Install using helm Eg. helm upgrade --install discoveredpolicy-daemon-charts-0.1.0.tgz -n accuknox-dev-discovered-policy-daemon Label-service \u00b6 Step 1 : Create namespace has accuknox-dev-label-service Eg. kubectl create ns accuknox-dev-label-service Step 2 : Install using helm Eg. helm upgrade --install label-service-chart-0.1.0.tgz -n accuknox-dev-label-service Label-daemon \u00b6 Step 1 : Create namespace has accuknox-dev-label-daemon Eg. kubectl create ns accuknox-dev-label-daemon Step 2 : Install using helm Eg. helm upgrade --install label-daemon-charts-1.0.4.tgz -n accuknox-dev-label-daemon Knox-auto-policy \u00b6 Step 1 : Create namespace has accuknox-dev-knoxautopolicy Eg. kubectl create ns accuknox-dev-knoxautopolicy Step 2 : Install using helm Eg. helm upgrade --install knox-auto-policy-chart.tgz -n accuknox-dev-knoxautopolicy","title":"How to install?"},{"location":"accuknox-onprem/core-components-install/#installing_helm","text":"This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.","title":"Installing Helm"},{"location":"accuknox-onprem/core-components-install/#from_the_binary_releases","text":"Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here..","title":"From the Binary Releases"},{"location":"accuknox-onprem/core-components-install/#add_accuknox_repository_to_install_core_components_helm_package","text":"helm repo add accuknox-onprem-core-components https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-core-components helm repo update helm search repo accuknox-onprem-core-components Follow the below order to install core components on k8s cluster","title":"Add accuknox repository to install Core Components helm package:"},{"location":"accuknox-onprem/core-components-install/#user-management-service","text":"Step 1 : Create namespace has accuknox-dev-user-mgmt Eg. kubectl create ns accuknox-dev-user-mgmt Step 2 : Install using helm Eg. helm upgrade --install user-management-service-0.1.0.tgz -n accuknox-dev-user-mgmt","title":"User-management-service:"},{"location":"accuknox-onprem/core-components-install/#agents-auth-service","text":"Step 1 : Create namespace has accuknox-dev-agents-auth-service Eg. kubectl create ns accuknox-dev-agents-auth-service Step 2 : Install using helm Eg. helm upgrade --install agents-auth-service-charts-0.1.0.tgz -n accuknox-dev-agents-auth-service","title":"Agents-Auth-Service"},{"location":"accuknox-onprem/core-components-install/#anomaly-detection-management","text":"Step 1 : Create namespace has accuknox-dev-ad-mgmt Eg. kubectl create ns accuknox-dev-ad-mgmt Step 2 : Install using helm Eg. helm upgrade --install anomaly-detection-mgmt-chart-0.1.0.tgz -n accuknox-dev-ad-mgmt","title":"Anomaly-detection-management"},{"location":"accuknox-onprem/core-components-install/#anomaly-detection-publisher-core","text":"Step 1 : Create namespace has accuknox-dev-ad-core Eg. kubectl create ns accuknox-dev-ad-core Step 2 : Install using helm Eg. helm upgrade --install anomaly-detection-publisher-core-chart-1.0.4.tgz -n accuknox-dev-ad-core","title":"Anomaly-detection-publisher-core"},{"location":"accuknox-onprem/core-components-install/#data-protection-mgmt","text":"Step 1 : Create namespace has accuknox-dev-dp-mgmt Eg. kubectl create ns accuknox-dev-dp-mgmt Step 2 : Install using helm Eg. helm upgrade --install data-protection-mgmt-0.1.0.tgz -n accuknox-dev-dp-mgmt","title":"Data-protection-mgmt"},{"location":"accuknox-onprem/core-components-install/#data-protection-core","text":"Step 1 : Create namespace has accuknox-dev-dp-core Eg. kubectl create ns accuknox-dev-dp-core Step 2 : Install using helm Eg. helm upgrade --install data-protection-core-1.0.4.tgz -n accuknox-dev-dp-core","title":"Data-protection-core"},{"location":"accuknox-onprem/core-components-install/#data-protection-consumer","text":"Step 1 : Create namespace has accuknox-dev-dp-core (Since ns already this step is optional) Eg. kubectl create ns accuknox-dev-dp-core Step 2 : Install using helm Eg. helm upgrade --install data-protection-consumer-0.1.0.tgz -n accuknox-dev-dp-core","title":"Data-protection-consumer"},{"location":"accuknox-onprem/core-components-install/#s3-audit-report-consumer","text":"Step 1 : Create namespace has accuknox-dev-s3-audit-reporter-consumer Eg. kubectl create ns accuknox-dev-s3-audit-reporter-consumer Step 2 : Install using helm Eg. helm upgrade --install s3-audit-reporter-consumer-charts-0.1.0.tgz -n accuknox-dev-s3-audit-reporter-consumer","title":"S3-audit-report-consumer"},{"location":"accuknox-onprem/core-components-install/#dp-db-audit-log-processor","text":"Step 1 : Create namespace has accuknox-dev-dp-core kubectl create ns accuknox-dev-dp-core Step 2 : Install using helm Eg. helm upgrade --install dp-db-audit-log-processor-chart-0.1.0.tgz -n accuknox-dev-dp-core","title":"Dp-db-audit-log-processor"},{"location":"accuknox-onprem/core-components-install/#data-classification-pipeline-consumer","text":"Step 1 : Create namespace has accuknox-dev-data-classification-pipeline-consumer Eg. kubectl create ns accuknox-dev-data-classification-pipeline-consumer Step 2 : Install using helm Eg. helm upgrade --install data-classification-pipeline-consumer-chart-0.1.0.tgz -n accuknox-dev-data-classification-pipeline-consumer","title":"Data-classification-pipeline-consumer"},{"location":"accuknox-onprem/core-components-install/#cluster-management-service","text":"Step 1 : Create namespace has accuknox-dev-cluster-mgmt Eg. kubectl create ns accuknox-dev-cluster-mgmt Step 2 : Install using helm Eg. helm upgrade --install cluster-management-service-chart-0.1.0.tgz -n accuknox-dev-cluster-mgmt","title":"Cluster-management-service"},{"location":"accuknox-onprem/core-components-install/#agent-data-collector","text":"Step 1 : Create namespace has accuknox-dev-adc Eg. kubectl create ns accuknox-dev-adc Step 2 : Install using helm Eg. helm upgrade --install agent-data-collector-charts-0.1.0.tgz -n accuknox-dev-adc","title":"Agent-data-collector"},{"location":"accuknox-onprem/core-components-install/#cluster-onboarding-service","text":"Step 1 : Create namespace has accuknox-dev-cluster-onboard Eg. kubectl create ns accuknox-dev-cluster-onboard Step 2 : Install using helm Eg. helm upgrade --install cluster-onboarding-service-0.1.0.tgz -n accuknox-dev-cluster-onboard","title":"Cluster-onboarding-service"},{"location":"accuknox-onprem/core-components-install/#cluster-entity-daemon","text":"Step 1 : Create namespace has accuknox-dev-cluster-entity-daemon Eg: kubectl create ns accuknox-dev-cluster-entity-daemon Step 2 : Install using helm Eg. helm upgrade --install cluster-entity-daemon-chart-0.1.0.tgz -n accuknox-dev-cluster-entity-daemon","title":"Cluster-entity-daemon"},{"location":"accuknox-onprem/core-components-install/#shared-informer-service","text":"Step 1 : Create namespace has accuknox-dev-shared-informer-service Eg. kubectl create ns accuknox-dev-shared-informer-service Step 2 : Install using helm Eg. helm upgrade --install shared-informer-service-chart-0.1.0.tgz -n accuknox-dev-shared-informer-service","title":"Shared-informer-service"},{"location":"accuknox-onprem/core-components-install/#data-pipeline-api","text":"Step 1 : Create namespace has accuknox-dev-datapipeline-api Eg. kubectl create ns accuknox-dev-datapipeline-api Step 2 : Install using helm Eg. helm upgrade --install data-pipeline-api-charts-0.1.0.tgz -n accuknox-dev-datapipeline-api","title":"Data-pipeline-api"},{"location":"accuknox-onprem/core-components-install/#datapipeline-temporal","text":"Step 1 : Create namespace has accuknox-dev-temporal Eg. kubectl create ns accuknox-dev-temporal Step 2 : Install using helm Eg. helm upgrade --install datapipeline-temporal-charts-0.1.0.tgz -n accuknox-dev-temporal","title":"Datapipeline-temporal"},{"location":"accuknox-onprem/core-components-install/#data-pipeline-samza-jobs","text":"Step 1 : Create namespace has accuknox-dev-samzajobs Eg. kubectl create ns accuknox-dev-samzajobs Step 2 : Install using helm Eg. helm upgrade --install datapipeline-samza-0.1.0.tgz -n accuknox-dev-samzajobs","title":"Data-pipeline-samza-jobs"},{"location":"accuknox-onprem/core-components-install/#feeder-grpc-server","text":"Step 1 : Create namespace has accuknox-dev-feeder-grpc-server Eg. kubectl create ns accuknox-dev-feeder-grpc-server Step 2 : Install using helm Eg. helm upgrade --install feeder-grpc-server-chart-0.1.0.tgz -n accuknox-dev-feeder-grpc-server Policy-service Step 1 : Create namespace has accuknox-dev-policy-service Eg. kubectl create ns accuknox-dev-policy-service Step 2 : Install using helm Eg. helm upgrade --install policy-service-charts-0.1.0.tgz -n accuknox-dev-policy-service","title":"Feeder-grpc-server"},{"location":"accuknox-onprem/core-components-install/#policy-daemon","text":"Step 1 : Create namespace has accuknox-dev-policy-daemon Eg. kubectl create ns accuknox-dev-policy-daemon Step 2 : Install using helm Eg. helm upgrade --install policy-daemon-charts-0.1.0.tgz -n accuknox-dev-policy-daemon","title":"Policy-daemon"},{"location":"accuknox-onprem/core-components-install/#policy-provider-service","text":"Step 1 : Create namespace has accuknox-dev-policy-provider-service Eg. kubectl create ns accuknox-dev-policy-provider-service Step 2 : Install using helm Eg. helm upgrade --install policy-provider-service-0.1.0.tgz -n accuknox-dev-policy-provider-service","title":"Policy-provider-service"},{"location":"accuknox-onprem/core-components-install/#workload-identity-daemon","text":"Step 1 : Create namespace has accuknox-dev-workload-identity-daemon Eg. kubectl create ns accuknox-dev-workload-identity-daemon Step 2 : Install using helm Eg. helm upgrade --install workload-identity-daemon-chart-0.1.0.tgz -n accuknox-dev-workload-identity-daemon","title":"Workload-identity-daemon"},{"location":"accuknox-onprem/core-components-install/#recommended-policy-daemon","text":"Step 1 : Create namespace has accuknox-dev-recommended-policy-daemon Eg. kubectl create ns accuknox-dev-recommended-policy-daemon Step 2 : Install using helm Eg. helm upgrade --install recommended-policy-daemon-1.0.4.tgz -n accuknox-dev-recommended-policy-daemon Discoveredpolicy-daemon Step 1 : Create namespace has accuknox-dev-discovered-policy-daemon Eg. kubectl create ns accuknox-dev-discovered-policy-daemon Step 2 : Install using helm Eg. helm upgrade --install discoveredpolicy-daemon-charts-0.1.0.tgz -n accuknox-dev-discovered-policy-daemon","title":"Recommended-policy-daemon"},{"location":"accuknox-onprem/core-components-install/#label-service","text":"Step 1 : Create namespace has accuknox-dev-label-service Eg. kubectl create ns accuknox-dev-label-service Step 2 : Install using helm Eg. helm upgrade --install label-service-chart-0.1.0.tgz -n accuknox-dev-label-service","title":"Label-service"},{"location":"accuknox-onprem/core-components-install/#label-daemon","text":"Step 1 : Create namespace has accuknox-dev-label-daemon Eg. kubectl create ns accuknox-dev-label-daemon Step 2 : Install using helm Eg. helm upgrade --install label-daemon-charts-1.0.4.tgz -n accuknox-dev-label-daemon","title":"Label-daemon"},{"location":"accuknox-onprem/core-components-install/#knox-auto-policy","text":"Step 1 : Create namespace has accuknox-dev-knoxautopolicy Eg. kubectl create ns accuknox-dev-knoxautopolicy Step 2 : Install using helm Eg. helm upgrade --install knox-auto-policy-chart.tgz -n accuknox-dev-knoxautopolicy","title":"Knox-auto-policy"},{"location":"accuknox-onprem/core-components-verify/","text":"","title":"How to verify?"},{"location":"accuknox-onprem/eck-install/","text":"Installing Helm \u00b6 This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases. From the Binary Releases \u00b6 Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install ECK helm package: \u00b6 helm repo add accuknox-onprem-logging https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-logging helm repo update helm search repo accuknox-onprem-logging helm pull accuknox-onprem-logging/eck-operator --untar helm pull accuknox-onprem-logging/elasticsearch --untar helm pull accuknox-onprem-logging/filebeat --untar helm pull accuknox-onprem-logging/kibana --untar Kubectl create namespace accuknox-logging helm install eck-operator eck-operator -n accuknox-logging helm install elasticsearch elasticsearch -n accuknox-logging helm install filebeat filebeat -n accuknox-logging helm install kibana kibana -n accuknox-logging How to verify kubectl get all -n accuknox-logging You can see all the pods are up and running. Configuration of filebeat After the verification you have to get the password for kibana to login The default username is \u201celastic\u201d Command to get the password: kubectl get secret elasticsearch-es-elastic-user -o go-template = '{{.data.elastic | base64decode}}' -n accuknox-logging It will give you a decoded secret name Once logged in with this username and password. Navigate to the Stack management->Index pattern->Create index pattern->filebeat-*->Next->choose timestamp","title":"How to install?"},{"location":"accuknox-onprem/eck-install/#installing_helm","text":"This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.","title":"Installing Helm"},{"location":"accuknox-onprem/eck-install/#from_the_binary_releases","text":"Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here..","title":"From the Binary Releases"},{"location":"accuknox-onprem/eck-install/#add_accuknox_repository_to_install_eck_helm_package","text":"helm repo add accuknox-onprem-logging https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-logging helm repo update helm search repo accuknox-onprem-logging helm pull accuknox-onprem-logging/eck-operator --untar helm pull accuknox-onprem-logging/elasticsearch --untar helm pull accuknox-onprem-logging/filebeat --untar helm pull accuknox-onprem-logging/kibana --untar Kubectl create namespace accuknox-logging helm install eck-operator eck-operator -n accuknox-logging helm install elasticsearch elasticsearch -n accuknox-logging helm install filebeat filebeat -n accuknox-logging helm install kibana kibana -n accuknox-logging How to verify kubectl get all -n accuknox-logging You can see all the pods are up and running. Configuration of filebeat After the verification you have to get the password for kibana to login The default username is \u201celastic\u201d Command to get the password: kubectl get secret elasticsearch-es-elastic-user -o go-template = '{{.data.elastic | base64decode}}' -n accuknox-logging It will give you a decoded secret name Once logged in with this username and password. Navigate to the Stack management->Index pattern->Create index pattern->filebeat-*->Next->choose timestamp","title":"Add accuknox repository to install ECK helm package:"},{"location":"accuknox-onprem/eck-purpose/","text":"Elastic Cloud on Kubernetes (ECK) extends the basic Kubernetes orchestration capabilities to support the setup and management of Elasticsearch, Kibana, Beats, on Kubernetes. With Elastic Cloud on Kubernetes we can streamline critical operations, such as: Managing and monitoring multiple clusters Scaling cluster capacity and storage Performing safe configuration changes through rolling upgrades Securing clusters with TLS certificates Setting up hot-warm-cold architectures with availability zone awareness Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels Taints - logging:true Labels - logging:true","title":"Purpose"},{"location":"accuknox-onprem/eck-verify/","text":"kubectl get all -n accuknox-logging You can see all the pods are up and running.","title":"How to verify?"},{"location":"accuknox-onprem/elastic-deploy/","text":"Temporal Operator Deployment \u00b6 1.Please create a namespace of your choice. Example : temporal-server \u00b6 kubectl create ns elastic-logging Note: Please use feeder-service namespace , if required. 2. Clone the git repository \u00b6 git clone -b dev https://github.com/accuknox/Accuknox-Logging Navigate into the directory that holds eck-operator folder. 3.Helm Install \u00b6 helm install eck-operator eck-operator/ -n <namespace> Navigate into the directory that holds elasticsearch folder. 4.Helm Install \u00b6 helm install elastisearch elasticsearch/ -n <namespace> Navigate into the directory that holds kibana folder. 5.Helm Install \u00b6 helm install kibana kibana/ -n <namespace> 6. Beats Setup: \u00b6 The agent will be spinned along with Filebeat running along as a sidecar. The filebeat configuration file in the package can be updated to specific Elastic instances, and logs can be viewed in Kibana . a. Elastic Configuration Parameters: \u00b6 The below Configuration parameters can be updated for elastic configuration. (If Default params needs to be modified) - name : ELASTICSEARCH_HOST value : https://<svc-name> - name : ELASTICSEARCH_PORT value : \"<svc-port>\" - name : ELASTICSEARCH_USERNAME value : \"elastic\" - name : ELASTICSEARCH_PASSWORD value : \"<elastic-password>\" To get elastic password kubectl get secret elasticsearch-es-elastic-user -o go-template = '{{.data.elastic | base64decode}}' -n namespace b. Command to be Used: \u00b6 kubectl set env deploy/feeder -n feeder-service ELASTICSEARCH_HOST = \u201dhttps://elasticsearch-es-http\u201d c. Update Log Path: \u00b6 To Update the Log path configured, please modify the below log input path under file beat inputs. filebeat.inputs : - type : container paths : - /log_output/cilium.log 7. Kibana Dashboard \u00b6 Once the filebeat starts listening, an index will be created or updated on the elastic configured and the pushed logs can be seen. In order to create a dashboard, you will need to build visualizations. Kibana has two panels for this One called Visualize and Another called Dashboard In order to create your dashboard, you will first create every individual visualization with the Visualize panel and save them. 8.Successful Installation \u00b6 kubectl get all -n <namespace> kubectl port-forward svc/kibana-kb-http 5601 :5601 All the pods should be up and running Kibana Ui with filebeat index should be seen (after beat installation)","title":"Elastic deploy"},{"location":"accuknox-onprem/elastic-deploy/#temporal_operator_deployment","text":"","title":"Temporal Operator Deployment"},{"location":"accuknox-onprem/elastic-deploy/#1please_create_a_namespace_of_your_choice_example_temporal-server","text":"kubectl create ns elastic-logging Note: Please use feeder-service namespace , if required.","title":"1.Please create a namespace of your choice. Example : temporal-server"},{"location":"accuknox-onprem/elastic-deploy/#2_clone_the_git_repository","text":"git clone -b dev https://github.com/accuknox/Accuknox-Logging Navigate into the directory that holds eck-operator folder.","title":"2. Clone the git repository"},{"location":"accuknox-onprem/elastic-deploy/#3helm_install","text":"helm install eck-operator eck-operator/ -n <namespace> Navigate into the directory that holds elasticsearch folder.","title":"3.Helm Install"},{"location":"accuknox-onprem/elastic-deploy/#4helm_install","text":"helm install elastisearch elasticsearch/ -n <namespace> Navigate into the directory that holds kibana folder.","title":"4.Helm Install"},{"location":"accuknox-onprem/elastic-deploy/#5helm_install","text":"helm install kibana kibana/ -n <namespace>","title":"5.Helm Install"},{"location":"accuknox-onprem/elastic-deploy/#6_beats_setup","text":"The agent will be spinned along with Filebeat running along as a sidecar. The filebeat configuration file in the package can be updated to specific Elastic instances, and logs can be viewed in Kibana .","title":"6. Beats Setup:"},{"location":"accuknox-onprem/elastic-deploy/#a_elastic_configuration_parameters","text":"The below Configuration parameters can be updated for elastic configuration. (If Default params needs to be modified) - name : ELASTICSEARCH_HOST value : https://<svc-name> - name : ELASTICSEARCH_PORT value : \"<svc-port>\" - name : ELASTICSEARCH_USERNAME value : \"elastic\" - name : ELASTICSEARCH_PASSWORD value : \"<elastic-password>\" To get elastic password kubectl get secret elasticsearch-es-elastic-user -o go-template = '{{.data.elastic | base64decode}}' -n namespace","title":"a. Elastic Configuration Parameters:"},{"location":"accuknox-onprem/elastic-deploy/#b_command_to_be_used","text":"kubectl set env deploy/feeder -n feeder-service ELASTICSEARCH_HOST = \u201dhttps://elasticsearch-es-http\u201d","title":"b. Command to be Used:"},{"location":"accuknox-onprem/elastic-deploy/#c_update_log_path","text":"To Update the Log path configured, please modify the below log input path under file beat inputs. filebeat.inputs : - type : container paths : - /log_output/cilium.log","title":"c. Update Log Path:"},{"location":"accuknox-onprem/elastic-deploy/#7_kibana_dashboard","text":"Once the filebeat starts listening, an index will be created or updated on the elastic configured and the pushed logs can be seen. In order to create a dashboard, you will need to build visualizations. Kibana has two panels for this One called Visualize and Another called Dashboard In order to create your dashboard, you will first create every individual visualization with the Visualize panel and save them.","title":"7. Kibana Dashboard"},{"location":"accuknox-onprem/elastic-deploy/#8successful_installation","text":"kubectl get all -n <namespace> kubectl port-forward svc/kibana-kb-http 5601 :5601 All the pods should be up and running Kibana Ui with filebeat index should be seen (after beat installation)","title":"8.Successful Installation"},{"location":"accuknox-onprem/elastic/","text":"ELK \u00b6 Elasticsearch is a search and analytics engine. It is an open source, full-text search and analysis engine, based on the Apache Lucene search engine. Logstash is a log aggregator that collects data from various input sources, executes different transformations and enhancements and then ships the data to various supported output destinations. Kibana is a visualization layer that works on top of Elasticsearch, providing users with the ability to analyze and visualize the data. And last but not least \u2014 Beats are lightweight agents that are installed on edge hosts to collect different types of data for forwarding into the stack. 1. Status of Elastic with On prem Feeder: \u00b6 Please run the below command to check if agent and dependent pods are up and running. kubectl get all -n feeder-service All the pods/services should be in Running state. 3. Metrics: \u00b6 Once the feeder agent starts running, check the logs using below command Kubectl logs \u2013f podname \u2013n feeder-service","title":"Elastic"},{"location":"accuknox-onprem/elastic/#elk","text":"Elasticsearch is a search and analytics engine. It is an open source, full-text search and analysis engine, based on the Apache Lucene search engine. Logstash is a log aggregator that collects data from various input sources, executes different transformations and enhancements and then ships the data to various supported output destinations. Kibana is a visualization layer that works on top of Elasticsearch, providing users with the ability to analyze and visualize the data. And last but not least \u2014 Beats are lightweight agents that are installed on edge hosts to collect different types of data for forwarding into the stack.","title":"ELK"},{"location":"accuknox-onprem/elastic/#1_status_of_elastic_with_on_prem_feeder","text":"Please run the below command to check if agent and dependent pods are up and running. kubectl get all -n feeder-service All the pods/services should be in Running state.","title":"1. Status of Elastic with On prem Feeder:"},{"location":"accuknox-onprem/elastic/#3_metrics","text":"Once the feeder agent starts running, check the logs using below command Kubectl logs \u2013f podname \u2013n feeder-service","title":"3. Metrics:"},{"location":"accuknox-onprem/hw-req-agent/","text":"Accuknpox Agents Resource Requirement \u00b6 Component Name Agents Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 4 Memory Per Node 4 Disk Size Per Node 50 Total CPU 12 Total Memory 12 Total Disk Size 150","title":"Hardware Pre-requisites"},{"location":"accuknox-onprem/hw-req-agent/#accuknpox_agents_resource_requirement","text":"Component Name Agents Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 4 Memory Per Node 4 Disk Size Per Node 50 Total CPU 12 Total Memory 12 Total Disk Size 150","title":"Accuknpox Agents Resource Requirement"},{"location":"accuknox-onprem/hw-req-core-components/","text":"Core Components Resource Requirement \u00b6 Component Name Core Components No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 12 Memory Per Node 24 Disk Size Per Node 50 Total CPU 36 Total Memory 72 Total Disk Size 150 Taints & Lables - Node Pool Name microservices","title":"Hardware Pre-requisites"},{"location":"accuknox-onprem/hw-req-core-components/#core_components_resource_requirement","text":"Component Name Core Components No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 12 Memory Per Node 24 Disk Size Per Node 50 Total CPU 36 Total Memory 72 Total Disk Size 150 Taints & Lables - Node Pool Name microservices","title":"Core Components Resource Requirement"},{"location":"accuknox-onprem/hw-req-logging/","text":"Logging Resource Requirement \u00b6 Component Name Logging No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 16 Disk Size Per Node 50 Total CPU 18 Total Memory 48 Total Disk Size 150 Taints & Lables logging:true Node Pool Name logging","title":"Hardware Pre-requisites"},{"location":"accuknox-onprem/hw-req-logging/#logging_resource_requirement","text":"Component Name Logging No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 16 Disk Size Per Node 50 Total CPU 18 Total Memory 48 Total Disk Size 150 Taints & Lables logging:true Node Pool Name logging","title":"Logging Resource Requirement"},{"location":"accuknox-onprem/hw-req-pre-requisites/","text":"Istio Resource Requirements \u00b6 Component Name Istio No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 2 Memory Per Node 4 Disk Size Per Node 50 Total CPU 6 Total Memory 12 Total Disk Size 150 MySQL Resource Requirement \u00b6 Component Name MySQL No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 16 Memory Per Node 20 Disk Size Per Node 50 Total CPU 48 Total Memory 60 Total Disk Size 150 Taints & Lables mysql:true Node Pool Name db-mysql Kafka Resource Requirement \u00b6 Component Name Kafka No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 16 Disk Size Per Node 50 Total CPU 18 Total Memory 48 Total Disk Size 150 Taints & Lables kafka:true Node Pool Name db-kafka Pinot Resource Requirement \u00b6 Component Name Pinot No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 20 Disk Size Per Node 50 Total CPU 18 Total Memory 60 Total Disk Size 150 Taints & Lables pinot:true Node Pool Name db-pinot","title":"Hardware Pre-requisites"},{"location":"accuknox-onprem/hw-req-pre-requisites/#istio_resource_requirements","text":"Component Name Istio No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 2 Memory Per Node 4 Disk Size Per Node 50 Total CPU 6 Total Memory 12 Total Disk Size 150","title":"Istio Resource Requirements"},{"location":"accuknox-onprem/hw-req-pre-requisites/#mysql_resource_requirement","text":"Component Name MySQL No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 16 Memory Per Node 20 Disk Size Per Node 50 Total CPU 48 Total Memory 60 Total Disk Size 150 Taints & Lables mysql:true Node Pool Name db-mysql","title":"MySQL Resource Requirement"},{"location":"accuknox-onprem/hw-req-pre-requisites/#kafka_resource_requirement","text":"Component Name Kafka No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 16 Disk Size Per Node 50 Total CPU 18 Total Memory 48 Total Disk Size 150 Taints & Lables kafka:true Node Pool Name db-kafka","title":"Kafka Resource Requirement"},{"location":"accuknox-onprem/hw-req-pre-requisites/#pinot_resource_requirement","text":"Component Name Pinot No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 20 Disk Size Per Node 50 Total CPU 18 Total Memory 60 Total Disk Size 150 Taints & Lables pinot:true Node Pool Name db-pinot","title":"Pinot Resource Requirement"},{"location":"accuknox-onprem/hw-req-prom-graf/","text":"Monitoring Resource Requirement \u00b6 Component Name Monitoring No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 10 Disk Size Per Node 50 Total CPU 18 Total Memory 30 Total Disk Size 150 Taints & Lables monitoring:true Node Pool Name monitoring","title":"Hardware Pre-requisites"},{"location":"accuknox-onprem/hw-req-prom-graf/#monitoring_resource_requirement","text":"Component Name Monitoring No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 10 Disk Size Per Node 50 Total CPU 18 Total Memory 30 Total Disk Size 150 Taints & Lables monitoring:true Node Pool Name monitoring","title":"Monitoring Resource Requirement"},{"location":"accuknox-onprem/istio-install/","text":"Istio Installion steps: \u00b6 Go to the Istio release page to download the installation file for your OS, or download and extract the latest release automatically (Linux or macOS): curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .10.0 TARGET_ARCH = x86_64 sh - Move to the Istio package directory. For example, if the package is istio-1.11.3: cd istio-1.10.0 Create a namespace istio-system for Istio components: kubectl create namespace istio-system Install the Istio base chart which contains cluster-wide resources used by the Istio control plane: helm install istio-base manifests/charts/base -n istio-system Install the Istio discovery chart which deploys the istiod control plane service: helm install istiod manifests/charts/istio-control/istio-discovery -n istio-system Install the Istio ingress gateway chart which contains the ingress gateway components: helm install istio-ingress manifests/charts/gateways/istio-ingress -n istio-system Verifying the installation: \u00b6 Ensure all Kubernetes pods in istio-system namespace are deployed and have a STATUS of Running: kubectl get pods -n istio-system Installing Gateway \u00b6 Along with creating a service mesh, Istio allows you to manage gateways, which are Envoy proxies running at the edge of the mesh, providing fine-grained control over traffic entering and leaving the mesh. Add accuknox repositories to install istio helm package: \u00b6 helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-prerequisites helm repo update helm search repo accuknox-onprem-prerequisites helm pull accuknox-onprem-prerequisites/istio-gateway-charts --untar Move to directory cd istio-gateway-charts Cert Manager \u00b6 Install cert-manager. Cert-manager will manage the certificates of gateway domains. Setup permissions \u00b6 When running on GKE (Google Kubernetes Engine), you might encounter a \u2018permission denied\u2019 error when creating some of the required resources. kubectl create clusterrolebinding cluster-admin-binding --clusterrole = cluster-admin --user = $( gcloud config get-value core/account ) Install Cert-manager \u00b6 kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.yaml Platform-istio-gateway \u00b6 Istio Gateway configurations for DNS This gateway config file defines the base API endpoints of the micro services under DNS This repository also contains necessary files to setup SSL for DNS (Refer issuer.yaml and cert.yaml) using cert-manager Create Gateway \u00b6 Find the Gateway IP INGRESS_HOST = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].ip}' ) This will give you a LoadBalancer IP echo ${ INGRESS_HOST } Create DNS \u00b6 Create A record for example (sample.example.com and keycloak.example.com) using LoadBalancer IP # Create a certificate Issuers, and ClusterIssuers, are Kubernetes resources that represent certificate authorities (CAs) that are able to generate signed certificates by honoring certificate signing requests. kubectl apply -f issuer.yaml kubectl get ClusterIssuer -n cert-manager # Should have Status as Ready A Certificate is a namespaced resource that references an Issuer or ClusterIssuer that determine what will be honoring the certificate request. kubectl apply -f cert.yaml kubectl get Certificate -n istio-system # Should have Status as Ready # Create gateway with SSL kubectl apply -f gateway-with-ssl.yaml ` [ No need to specify namespace ] # Apply Virtual Service A VirtualService defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset/version of it) defined in the registry. kubectl apply -f backend-api/virtual-service.yaml [No need to specify namespace] kubectl apply -f keycloak/virtual-service.yaml # [No need to specify namespace]","title":"How to install?"},{"location":"accuknox-onprem/istio-install/#istio_installion_steps","text":"Go to the Istio release page to download the installation file for your OS, or download and extract the latest release automatically (Linux or macOS): curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .10.0 TARGET_ARCH = x86_64 sh - Move to the Istio package directory. For example, if the package is istio-1.11.3: cd istio-1.10.0 Create a namespace istio-system for Istio components: kubectl create namespace istio-system Install the Istio base chart which contains cluster-wide resources used by the Istio control plane: helm install istio-base manifests/charts/base -n istio-system Install the Istio discovery chart which deploys the istiod control plane service: helm install istiod manifests/charts/istio-control/istio-discovery -n istio-system Install the Istio ingress gateway chart which contains the ingress gateway components: helm install istio-ingress manifests/charts/gateways/istio-ingress -n istio-system","title":"Istio Installion steps:"},{"location":"accuknox-onprem/istio-install/#verifying_the_installation","text":"Ensure all Kubernetes pods in istio-system namespace are deployed and have a STATUS of Running: kubectl get pods -n istio-system","title":"Verifying the installation:"},{"location":"accuknox-onprem/istio-install/#installing_gateway","text":"Along with creating a service mesh, Istio allows you to manage gateways, which are Envoy proxies running at the edge of the mesh, providing fine-grained control over traffic entering and leaving the mesh.","title":"Installing Gateway"},{"location":"accuknox-onprem/istio-install/#add_accuknox_repositories_to_install_istio_helm_package","text":"helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-prerequisites helm repo update helm search repo accuknox-onprem-prerequisites helm pull accuknox-onprem-prerequisites/istio-gateway-charts --untar Move to directory cd istio-gateway-charts","title":"Add accuknox repositories to install istio helm package:"},{"location":"accuknox-onprem/istio-install/#cert_manager","text":"Install cert-manager. Cert-manager will manage the certificates of gateway domains.","title":"Cert Manager"},{"location":"accuknox-onprem/istio-install/#setup_permissions","text":"When running on GKE (Google Kubernetes Engine), you might encounter a \u2018permission denied\u2019 error when creating some of the required resources. kubectl create clusterrolebinding cluster-admin-binding --clusterrole = cluster-admin --user = $( gcloud config get-value core/account )","title":"Setup permissions"},{"location":"accuknox-onprem/istio-install/#install_cert-manager","text":"kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.yaml","title":"Install Cert-manager"},{"location":"accuknox-onprem/istio-install/#platform-istio-gateway","text":"Istio Gateway configurations for DNS This gateway config file defines the base API endpoints of the micro services under DNS This repository also contains necessary files to setup SSL for DNS (Refer issuer.yaml and cert.yaml) using cert-manager","title":"Platform-istio-gateway"},{"location":"accuknox-onprem/istio-install/#create_gateway","text":"Find the Gateway IP INGRESS_HOST = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].ip}' ) This will give you a LoadBalancer IP echo ${ INGRESS_HOST }","title":"Create Gateway"},{"location":"accuknox-onprem/istio-install/#create_dns","text":"Create A record for example (sample.example.com and keycloak.example.com) using LoadBalancer IP # Create a certificate Issuers, and ClusterIssuers, are Kubernetes resources that represent certificate authorities (CAs) that are able to generate signed certificates by honoring certificate signing requests. kubectl apply -f issuer.yaml kubectl get ClusterIssuer -n cert-manager # Should have Status as Ready A Certificate is a namespaced resource that references an Issuer or ClusterIssuer that determine what will be honoring the certificate request. kubectl apply -f cert.yaml kubectl get Certificate -n istio-system # Should have Status as Ready # Create gateway with SSL kubectl apply -f gateway-with-ssl.yaml ` [ No need to specify namespace ] # Apply Virtual Service A VirtualService defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset/version of it) defined in the registry. kubectl apply -f backend-api/virtual-service.yaml [No need to specify namespace] kubectl apply -f keycloak/virtual-service.yaml # [No need to specify namespace]","title":"Create DNS"},{"location":"accuknox-onprem/istio-purpose/","text":"Istio is an open source service mesh that layers transparently onto existing distributed applications. Istio\u2019s powerful features provide a uniform and more efficient way to secure, connect, and monitor services. Istio is the path to load balancing, service-to-service authentication, and monitoring \u2013 with few or no service code changes. Its powerful control plane brings vital features, including: Secure service-to-service communication in a cluster with TLS encryption, strong identity based authentication and authorization Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic Fine-grained control of traffic behavior with rich routing rules, retries, failovers, and fault injection A pluggable policy layer and configuration API supporting access controls, rate limits and quotas Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress For more reference: click here..","title":"Purpose"},{"location":"accuknox-onprem/istio-verify/","text":"","title":"How to verify?"},{"location":"accuknox-onprem/kafka-install/","text":"Note: \u00b6 Kafka Operator Deployment 1. Don't change the namespace name because if you change the namespace - You need to change the service name , If the service name is changed ,then you need to change the microservice configmap files . eg: app.yaml. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels Taints - kafka:true Labels - kafka:true Installing Helm \u00b6 This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases. From the Binary Releases \u00b6 Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install strimzi-kafka-operator helm package: helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-prerequisites helm repo update helm search repo accuknox-onprem-prerequisites helm pull accuknox-onprem-prerequisites/strimzi-kafka-operator --untar kubectl create namespace accuknox-dev-kafka kubectl config set-context --current --namespace = accuknox-dev-kafka helm install dev-kafka strimzi-kafka-operator Check the Pods deployment kubectl get pods -n accuknox-dev-kafka Extract the connectivity information. Get bootstrap server endpoint \u00b6 kubectl get kafka dev-kafka -o jsonpath = '{.status.listeners[?(@.type==\"external\")].bootstrapServers}' -n accuknox-dev-kafka Get CA \u00b6 kubectl get secret dev-kafka-cluster-ca-cert -o jsonpath = '{.data.ca\\.p12}' -n accuknox-dev-kafka | base64 -d > ca.p12 Get CA Password \u00b6 kubectl get secret dev-kafka-cluster-ca-cert -o jsonpath = '{.data.ca\\.password}' -n accuknox-dev-kafka | base64 -d > ca.password Get User Cert \u00b6 kubectl get secret/node-event-feeder-common -n accuknox-dev-kafka -o jsonpath = '{.data.user\\.p12}' | base64 -d > user.p12 Get user password \u00b6 kubectl get secret/node-event-feeder-common -n accuknox-dev-kafka -o jsonpath = '{.data.user\\.password}' | base64 -d > user.password Convert user.p12 into base64 \u00b6 cat user.p12 | base64 > user.p12.base64 Convert ca.p12 into base64 \u00b6 cat ca.p12 | base64 > ca.p12.base64 Convert ca.password into base64 \u00b6 cat ca.password | base64 > ca.password.base64 Convert user.password into base64 \u00b6 cat user.password | base64 > user.password.base64 Convert p12 to pem \u00b6 openssl pkcs12 -in ca.p12 -out ca.pem Convert ca.pem to base64 \u00b6 cat ca.pem | base64 > ca.pem.base64 Note: ca.p12, ca.password, user.p12 and user.password are required to be used in Java based applications. For Go based applications, use ca.pem, user.p12 and user.password. For use in Kubernetes, use the base64 versions of respective files. FQDN (K8\u2019s Service name) Value for Internal Cluster application connectivity. FQDN : dev-kafka-kafka-bootstrap.accuknox-dev-kafka.svc.cluster.local:9092","title":"How to install?"},{"location":"accuknox-onprem/kafka-install/#note","text":"Kafka Operator Deployment 1. Don't change the namespace name because if you change the namespace - You need to change the service name , If the service name is changed ,then you need to change the microservice configmap files . eg: app.yaml. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels Taints - kafka:true Labels - kafka:true","title":"Note:"},{"location":"accuknox-onprem/kafka-install/#installing_helm","text":"This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.","title":"Installing Helm"},{"location":"accuknox-onprem/kafka-install/#from_the_binary_releases","text":"Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install strimzi-kafka-operator helm package: helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-prerequisites helm repo update helm search repo accuknox-onprem-prerequisites helm pull accuknox-onprem-prerequisites/strimzi-kafka-operator --untar kubectl create namespace accuknox-dev-kafka kubectl config set-context --current --namespace = accuknox-dev-kafka helm install dev-kafka strimzi-kafka-operator Check the Pods deployment kubectl get pods -n accuknox-dev-kafka Extract the connectivity information.","title":"From the Binary Releases"},{"location":"accuknox-onprem/kafka-install/#get_bootstrap_server_endpoint","text":"kubectl get kafka dev-kafka -o jsonpath = '{.status.listeners[?(@.type==\"external\")].bootstrapServers}' -n accuknox-dev-kafka","title":"Get bootstrap server endpoint"},{"location":"accuknox-onprem/kafka-install/#get_ca","text":"kubectl get secret dev-kafka-cluster-ca-cert -o jsonpath = '{.data.ca\\.p12}' -n accuknox-dev-kafka | base64 -d > ca.p12","title":"Get CA"},{"location":"accuknox-onprem/kafka-install/#get_ca_password","text":"kubectl get secret dev-kafka-cluster-ca-cert -o jsonpath = '{.data.ca\\.password}' -n accuknox-dev-kafka | base64 -d > ca.password","title":"Get CA Password"},{"location":"accuknox-onprem/kafka-install/#get_user_cert","text":"kubectl get secret/node-event-feeder-common -n accuknox-dev-kafka -o jsonpath = '{.data.user\\.p12}' | base64 -d > user.p12","title":"Get User Cert"},{"location":"accuknox-onprem/kafka-install/#get_user_password","text":"kubectl get secret/node-event-feeder-common -n accuknox-dev-kafka -o jsonpath = '{.data.user\\.password}' | base64 -d > user.password","title":"Get user password"},{"location":"accuknox-onprem/kafka-install/#convert_userp12_into_base64","text":"cat user.p12 | base64 > user.p12.base64","title":"Convert user.p12 into base64"},{"location":"accuknox-onprem/kafka-install/#convert_cap12_into_base64","text":"cat ca.p12 | base64 > ca.p12.base64","title":"Convert ca.p12 into base64"},{"location":"accuknox-onprem/kafka-install/#convert_capassword_into_base64","text":"cat ca.password | base64 > ca.password.base64","title":"Convert ca.password into base64"},{"location":"accuknox-onprem/kafka-install/#convert_userpassword_into_base64","text":"cat user.password | base64 > user.password.base64","title":"Convert user.password into base64"},{"location":"accuknox-onprem/kafka-install/#convert_p12_to_pem","text":"openssl pkcs12 -in ca.p12 -out ca.pem","title":"Convert p12 to pem"},{"location":"accuknox-onprem/kafka-install/#convert_capem_to_base64","text":"cat ca.pem | base64 > ca.pem.base64 Note: ca.p12, ca.password, user.p12 and user.password are required to be used in Java based applications. For Go based applications, use ca.pem, user.p12 and user.password. For use in Kubernetes, use the base64 versions of respective files. FQDN (K8\u2019s Service name) Value for Internal Cluster application connectivity. FQDN : dev-kafka-kafka-bootstrap.accuknox-dev-kafka.svc.cluster.local:9092","title":"Convert ca.pem to base64"},{"location":"accuknox-onprem/kafka-purpose/","text":"Strimzi simplifies the process of running Apache Kafka in a Kubernetes cluster. Features \u00b6 The underlying data stream-processing capabilities and component architecture of Kafka can deliver: Microservices and other applications to share data with extremely high throughput and low latency Message ordering guarantees Message rewind/replay from data storage to reconstruct an application state Message compaction to remove old records when using a key-value log Horizontal scalability in a cluster configuration Replication of data to control fault tolerance Retention of high volumes of data for immediate access For more reference: click here..","title":"Purpose"},{"location":"accuknox-onprem/kafka-purpose/#features","text":"The underlying data stream-processing capabilities and component architecture of Kafka can deliver: Microservices and other applications to share data with extremely high throughput and low latency Message ordering guarantees Message rewind/replay from data storage to reconstruct an application state Message compaction to remove old records when using a key-value log Horizontal scalability in a cluster configuration Replication of data to control fault tolerance Retention of high volumes of data for immediate access For more reference: click here..","title":"Features"},{"location":"accuknox-onprem/kafka-verify/","text":"","title":"How to verify?"},{"location":"accuknox-onprem/loki-install/","text":"Installing Helm \u00b6 This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases. From the Binary Releases \u00b6 Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install Loki helm package: helm repo add accuknox-onprem-logging https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-logging helm repo update helm search repo accuknox-onprem-logging helm pull accuknox-onprem-logging/loki-stack --untar kubectl create namespace accuknox-loki-logging kubectl config set-context --current --namespace = accuknox-loki-logging helm install loki loki-stack/","title":"How to install?"},{"location":"accuknox-onprem/loki-install/#installing_helm","text":"This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.","title":"Installing Helm"},{"location":"accuknox-onprem/loki-install/#from_the_binary_releases","text":"Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install Loki helm package: helm repo add accuknox-onprem-logging https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-logging helm repo update helm search repo accuknox-onprem-logging helm pull accuknox-onprem-logging/loki-stack --untar kubectl create namespace accuknox-loki-logging kubectl config set-context --current --namespace = accuknox-loki-logging helm install loki loki-stack/","title":"From the Binary Releases"},{"location":"accuknox-onprem/loki-purpose/","text":"Loki is a log aggregation tool, and it is the core of a fully-featured logging stack. Loki is a datastore optimized for efficiently holding log data. A Loki-based logging stack consists of 3 components: Fluent bit is daemonset its running in each node to get the logs. Promtail is the agent, responsible for gathering logs and sending them to Loki. Loki is the main server, responsible for storing logs and processing queries.","title":"Purpose"},{"location":"accuknox-onprem/loki-verify/","text":"Check the Pods deployment kubectl get pods -n accuknox-loki-logging Add endpoint in grafana datasourceCheck the Pods deployment kubectl get pods -n accuknox-loki-logging Add endpoint in grafana datasource","title":"Loki verify"},{"location":"accuknox-onprem/mysql-install/","text":"Note: \u00b6 Don't change the namespace name because if you change the namespace - You need to change the service name , If the service name is changed ,then you need to change the microservice configmap files . eg: app.yaml. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels Taints - mysql:true Labels - mysql:true Installing Helm \u00b6 This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases. From the Binary Releases \u00b6 Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install Mysql Percona helm package: helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-prerequisites helm repo update helm search repo accuknox-onprem-prerequisites helm pull accuknox-onprem-prerequisites/mysql-chart --untar kubectl create namespace accuknox-dev-mysql kubectl config set-context $( kubectl config current-context ) --namespace = accuknox-dev-mysql cd mysql-chart kubectl apply -f bundle.yaml kubectl apply -f cr.yaml kubectl apply -f secrets.yaml kubectl apply -f ssl-secrets.yaml kubectl apply -f backup-s3.yaml After following steps the above steps, you will see a similar image as above Run a sanitary test with below commands at the mysql namespace kubectl run -i --rm --tty percona-client --image = percona:8.0 --restart = Never -- bash -il mysql -h accuknox-dev-mysql-haproxy -uroot -proot_password Update the passwords in secret.yaml file and run below command kubectl apply -f secrets.yaml Optional To configure backup with gcs add the HMAC keys in backup-s3.yaml, change the bucket name in cr.yaml and cron can be changed as required cr.yaml files. FQDN: For K8\u2019s Service name accuknox-dev-mysql-haproxy.accuknox-dev-mysql.svc.cluster.local \u00b6","title":"How to install?"},{"location":"accuknox-onprem/mysql-install/#note","text":"Don't change the namespace name because if you change the namespace - You need to change the service name , If the service name is changed ,then you need to change the microservice configmap files . eg: app.yaml. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels Taints - mysql:true Labels - mysql:true","title":"Note:"},{"location":"accuknox-onprem/mysql-install/#installing_helm","text":"This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.","title":"Installing Helm"},{"location":"accuknox-onprem/mysql-install/#from_the_binary_releases","text":"Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install Mysql Percona helm package: helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-prerequisites helm repo update helm search repo accuknox-onprem-prerequisites helm pull accuknox-onprem-prerequisites/mysql-chart --untar kubectl create namespace accuknox-dev-mysql kubectl config set-context $( kubectl config current-context ) --namespace = accuknox-dev-mysql cd mysql-chart kubectl apply -f bundle.yaml kubectl apply -f cr.yaml kubectl apply -f secrets.yaml kubectl apply -f ssl-secrets.yaml kubectl apply -f backup-s3.yaml After following steps the above steps, you will see a similar image as above Run a sanitary test with below commands at the mysql namespace kubectl run -i --rm --tty percona-client --image = percona:8.0 --restart = Never -- bash -il mysql -h accuknox-dev-mysql-haproxy -uroot -proot_password Update the passwords in secret.yaml file and run below command kubectl apply -f secrets.yaml Optional To configure backup with gcs add the HMAC keys in backup-s3.yaml, change the bucket name in cr.yaml and cron can be changed as required cr.yaml files. FQDN: For K8\u2019s Service name","title":"From the Binary Releases"},{"location":"accuknox-onprem/mysql-install/#accuknox-dev-mysql-haproxyaccuknox-dev-mysqlsvcclusterlocal","text":"","title":"accuknox-dev-mysql-haproxy.accuknox-dev-mysql.svc.cluster.local"},{"location":"accuknox-onprem/mysql-purpose/","text":"Percona Distribution for Mysql Operator is an open-source drop in replacement for MySQL Enterprise with synchronous replication running on Kubernetes. It automates the deployment and management of the members in your Percona XtraDB Cluster environment. It can be used to instantiate a new Percona XtraDB Cluster, or to scale an existing environment. Consult the documentation on the Percona Distribution for Mysql Operator for complete details on capabilities and options. Supported Features \u00b6 Scale Your Cluster change the size parameter to add or remove members of the cluster. Three is the minimum recommended size for a functioning cluster. Automate Your Backups configure cluster backups to run on a scheduled basis. Backups can be stored on a persistent volume or S3-compatible storage. Leverage Point-in-time recovery to reduce RPO/RTO. Proxy integration choose HAProxy or ProxySQL as a proxy in front of the Percona XtraDB Cluster. Proxies are deployed and configured automatically with the Operator. Common Configurations \u00b6 Set Resource Limits - set limitation on requests to CPU and memory resources. Customize Storage - set the desired Storage Class and Access Mode for your database cluster data. Control Scheduling - define how your PXC Pods are scheduled onto the K8S cluster with tolerations, pod disruption budgets, node selector and affinity settings. Automatic synchronization of MySQL users with ProxySQL Fully automated minor version updates (Smart Update) Update Reader members before Writer member at cluster upgrades For more reference: click here..","title":"Purpose"},{"location":"accuknox-onprem/mysql-purpose/#supported_features","text":"Scale Your Cluster change the size parameter to add or remove members of the cluster. Three is the minimum recommended size for a functioning cluster. Automate Your Backups configure cluster backups to run on a scheduled basis. Backups can be stored on a persistent volume or S3-compatible storage. Leverage Point-in-time recovery to reduce RPO/RTO. Proxy integration choose HAProxy or ProxySQL as a proxy in front of the Percona XtraDB Cluster. Proxies are deployed and configured automatically with the Operator.","title":"Supported Features"},{"location":"accuknox-onprem/mysql-purpose/#common_configurations","text":"Set Resource Limits - set limitation on requests to CPU and memory resources. Customize Storage - set the desired Storage Class and Access Mode for your database cluster data. Control Scheduling - define how your PXC Pods are scheduled onto the K8S cluster with tolerations, pod disruption budgets, node selector and affinity settings. Automatic synchronization of MySQL users with ProxySQL Fully automated minor version updates (Smart Update) Update Reader members before Writer member at cluster upgrades For more reference: click here..","title":"Common Configurations"},{"location":"accuknox-onprem/mysql-verify/","text":"","title":"How to verify?"},{"location":"accuknox-onprem/pinot-install/","text":"Note: \u00b6 Pinot Operator Deployment 1. Don't change the namespace name because if you change the namespace - You need to change the service name , If the service name is changed ,then you need to change the microservice configmap files . eg: app.yaml. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels Taints - pinot:true Labels - pinot:true Installing Helm \u00b6 This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases. From the Binary Releases \u00b6 Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install Pinot helm package helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-prerequisites helm repo update helm search repo accuknox-onprem-prerequisites helm pull accuknox-onprem-prerequisites/pinot --untar kubectl create namespace accuknox-dev-pinot kubectl config set-context \u2013current --namespace = accuknox-dev-pinot helm install accuknox-dev-pinot pinot kubectl get all","title":"How to install?"},{"location":"accuknox-onprem/pinot-install/#note","text":"Pinot Operator Deployment 1. Don't change the namespace name because if you change the namespace - You need to change the service name , If the service name is changed ,then you need to change the microservice configmap files . eg: app.yaml. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels Taints - pinot:true Labels - pinot:true","title":"Note:"},{"location":"accuknox-onprem/pinot-install/#installing_helm","text":"This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.","title":"Installing Helm"},{"location":"accuknox-onprem/pinot-install/#from_the_binary_releases","text":"Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install Pinot helm package helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-prerequisites helm repo update helm search repo accuknox-onprem-prerequisites helm pull accuknox-onprem-prerequisites/pinot --untar kubectl create namespace accuknox-dev-pinot kubectl config set-context \u2013current --namespace = accuknox-dev-pinot helm install accuknox-dev-pinot pinot kubectl get all","title":"From the Binary Releases"},{"location":"accuknox-onprem/pinot-purpose/","text":"Pinot is a real-time distributed OLAP datastore, purpose-built to provide ultra low-latency analytics, even at extremely high throughput. It can ingest directly from streaming data sources - such as Apache Kafka and Amazon Kinesis - and make the events available for querying instantly. It can also ingest from batch data sources such as Hadoop HDFS, Amazon S3, Azure ADLS, and Google Cloud Storage. At the heart of the system is a columnar store, with several smart indexing and pre-aggregation techniques for low latency. This makes Pinot the most perfect fit for user-facing realtime analytics. At the same time, Pinot is also a great choice for other analytical use-cases, such as internal dashboards, anomaly detection, and ad-hoc data exploration. For more reference: click here..","title":"Purpose"},{"location":"accuknox-onprem/pinot-verify/","text":"","title":"How to verify?"},{"location":"accuknox-onprem/prom-graf-install/","text":"Note: \u00b6 Prometheus Grafana Deployment Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Tolerations and Node Selector Tolerations: - key:\u201dmonitoring\u201d operator: \u201cEqual\u201d value: \u201ctrue\u201d effect: \u201cNoSchedule\u201d Nodeselector: monitoring: \u201ctrue\u201d Installing Helm \u00b6 This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases. From the Binary Releases \u00b6 Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install Prometheus & Grafana helm package: helm repo add accuknox-onprem-monitoring https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-monitoring helm repo update helm search repo accuknox-onprem-monitoring helm pull accuknox-onprem-monitoring/grafana-prometheus-stack --untar kubectl create namespace accuknox-monitoring kubectl config set-context --current --namespace = accuknox-monitoring helm install prometheusmetrics grafana-prometheus-stack Check the Pods deployment: kubectl get pods -n accuknox-monitoring Default Username & Password for Grafana: User: admin Password: prom-operator","title":"How to install?"},{"location":"accuknox-onprem/prom-graf-install/#note","text":"Prometheus Grafana Deployment Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Tolerations and Node Selector Tolerations: - key:\u201dmonitoring\u201d operator: \u201cEqual\u201d value: \u201ctrue\u201d effect: \u201cNoSchedule\u201d Nodeselector: monitoring: \u201ctrue\u201d","title":"Note:"},{"location":"accuknox-onprem/prom-graf-install/#installing_helm","text":"This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.","title":"Installing Helm"},{"location":"accuknox-onprem/prom-graf-install/#from_the_binary_releases","text":"Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install Prometheus & Grafana helm package: helm repo add accuknox-onprem-monitoring https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-monitoring helm repo update helm search repo accuknox-onprem-monitoring helm pull accuknox-onprem-monitoring/grafana-prometheus-stack --untar kubectl create namespace accuknox-monitoring kubectl config set-context --current --namespace = accuknox-monitoring helm install prometheusmetrics grafana-prometheus-stack Check the Pods deployment: kubectl get pods -n accuknox-monitoring Default Username & Password for Grafana: User: admin Password: prom-operator","title":"From the Binary Releases"},{"location":"accuknox-onprem/prom-graf-purpose/","text":"Prometheus is an open-source systems monitoring and alerting toolkit.Prometheus collects and stores its metrics as time series data. Features: PromQL, a flexible query language to leverage this dimensionality Time series collection happens via a pull model over HTTP Pushing time series is supported via an intermediary gateway Targets are discovered via service discovery or static configuration Multiple modes of graphing and dashboarding support Dependencies: By default this chart installs additional, dependent charts: Kube-state-metrics prometheus-node-exporter Grafana","title":"Purpose"},{"location":"accuknox-onprem/prom-graf-verify/","text":"kubectl get all -n accuknox-monitoring You can see all the pods are up and running.","title":"How to verify?"},{"location":"accuknox-onprem/support/","text":"Please post your issues on support.accuknox.com","title":"Support"},{"location":"accuknox-onprem/sw-req-pre-requisites/","text":"","title":"Sw req pre requisites"},{"location":"accuknox-onprem/temporal-deploy/","text":"Temporal Operator Deployment \u00b6 1. Please create a namespace of your choice. Example : temporal-server \u00b6 kubectl create ns temporal-server 2. Clone the git repository \u00b6 git clone https://github.com/temporalio/helm-charts.git Navigate into the directory that holds helm-charts folder. 3.Helm Install \u00b6 helm install temporal -n <namespace> . If Prometheus/ Grafana is not required, please use the below command. helm install --set server.replicaCount = 1 --set cassandra.config.cluster_size = 1 --set prometheus.enabled = false --set grafana.enabled = false --set elasticsearch.enabled = false temporal . --timeout 15m -n <namespace> kubectl get all -n <namespace> 4 .Set the Default Namespace \u00b6 kubectl exec -n <namespace> -it pod/temporaltest-admintools-<pod-id> -- /bin/bash tctl --ns default n re 5 .Successful Installation \u00b6 Port-forward the temporal-web (:8088) pod to view the temporal workflows UI. kubectl port-forward svc/temporaltest-web 8088 :8088 -n <namespace>","title":"Temporal deploy"},{"location":"accuknox-onprem/temporal-deploy/#temporal_operator_deployment","text":"","title":"Temporal Operator Deployment"},{"location":"accuknox-onprem/temporal-deploy/#1_please_create_a_namespace_of_your_choice_example_temporal-server","text":"kubectl create ns temporal-server","title":"1. Please create a namespace of your choice. Example : temporal-server"},{"location":"accuknox-onprem/temporal-deploy/#2_clone_the_git_repository","text":"git clone https://github.com/temporalio/helm-charts.git Navigate into the directory that holds helm-charts folder.","title":"2. Clone the git repository"},{"location":"accuknox-onprem/temporal-deploy/#3helm_install","text":"helm install temporal -n <namespace> . If Prometheus/ Grafana is not required, please use the below command. helm install --set server.replicaCount = 1 --set cassandra.config.cluster_size = 1 --set prometheus.enabled = false --set grafana.enabled = false --set elasticsearch.enabled = false temporal . --timeout 15m -n <namespace> kubectl get all -n <namespace>","title":"3.Helm Install"},{"location":"accuknox-onprem/temporal-deploy/#4_set_the_default_namespace","text":"kubectl exec -n <namespace> -it pod/temporaltest-admintools-<pod-id> -- /bin/bash tctl --ns default n re","title":"4 .Set the Default Namespace"},{"location":"accuknox-onprem/temporal-deploy/#5_successful_installation","text":"Port-forward the temporal-web (:8088) pod to view the temporal workflows UI. kubectl port-forward svc/temporaltest-web 8088 :8088 -n <namespace>","title":"5 .Successful Installation"},{"location":"accuknox-onprem/temporal/","text":"Temporal \u00b6 Temporal is an orchestration platform for microservices and a workflow engine that runs in a loop until the workflow is complete. The major advantages of temporal include Handling intermittent failures Re-running the failed operations until success. Supporting long running operations. Running multiple operations parallelly. Temporal are written in two types of special purpose functions: 1. What are workflows ? \u00b6 Workflows can be seen as a set of tasks that has a specific goal to achieve and it will run until the goal is achieved. Workflow has various timeout options in order to stop the workflow if it runs for a longer period of time. 2.What are activities ? \u00b6 Activities contain the business logic of the user application. It is invoked via workflow and task queues. Task queues are used to store activities in a queue and a worker comes and picks up an activity to get it done. 3.How are temporal workflows used in AccuKnox ? \u00b6 There are various components in Accuknox such as Network, System, Anomaly Detection and Data protection and these components send logs to kafka topic. The role of the workflow comes here where there are specific workflows for each component and they continuously run, scanning for logs from kafka. The workflow runs without a pause since the logs from the specific components comes every second and it has to capture it and process the logs such that it can send it to different integration channels.","title":"Temporal"},{"location":"accuknox-onprem/temporal/#temporal","text":"Temporal is an orchestration platform for microservices and a workflow engine that runs in a loop until the workflow is complete. The major advantages of temporal include Handling intermittent failures Re-running the failed operations until success. Supporting long running operations. Running multiple operations parallelly. Temporal are written in two types of special purpose functions:","title":"Temporal"},{"location":"accuknox-onprem/temporal/#1_what_are_workflows","text":"Workflows can be seen as a set of tasks that has a specific goal to achieve and it will run until the goal is achieved. Workflow has various timeout options in order to stop the workflow if it runs for a longer period of time.","title":"1. What are workflows ?"},{"location":"accuknox-onprem/temporal/#2what_are_activities","text":"Activities contain the business logic of the user application. It is invoked via workflow and task queues. Task queues are used to store activities in a queue and a worker comes and picks up an activity to get it done.","title":"2.What are activities ?"},{"location":"accuknox-onprem/temporal/#3how_are_temporal_workflows_used_in_accuknox","text":"There are various components in Accuknox such as Network, System, Anomaly Detection and Data protection and these components send logs to kafka topic. The role of the workflow comes here where there are specific workflows for each component and they continuously run, scanning for logs from kafka. The workflow runs without a pause since the logs from the specific components comes every second and it has to capture it and process the logs such that it can send it to different integration channels.","title":"3.How are temporal workflows used in AccuKnox ?"},{"location":"accuknox-onprem/ui-deploy/","text":"Add accuknox repository to install UI on VM \u00b6 helm repo add accuknox-onprem-ui https:/USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-ui helm repo update helm search repo accuknox-onprem-ui helm pull accuknox-ui/ui-build --untar Step 1 : Install nginx application on VM Step 2: Configure the certmanager(https)(eg: letsencrypt) Step 3: tar -xvf build.tar.gz Step 4: sudo cp -rvf build/* /usr/share/nginx/html/ Step 5: Start the service","title":"Ui deploy"},{"location":"accuknox-onprem/ui-deploy/#add_accuknox_repository_to_install_ui_on_vm","text":"helm repo add accuknox-onprem-ui https:/USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-ui helm repo update helm search repo accuknox-onprem-ui helm pull accuknox-ui/ui-build --untar Step 1 : Install nginx application on VM Step 2: Configure the certmanager(https)(eg: letsencrypt) Step 3: tar -xvf build.tar.gz Step 4: sudo cp -rvf build/* /usr/share/nginx/html/ Step 5: Start the service","title":"Add accuknox repository to install UI on VM"},{"location":"accuknox-onprem/what-is-ak-on-prem/","text":"Overview \u00b6 The best Kubernetes architecture for your organization depends on your needs and goals. Kubernetes is often described as a cloud-native technology, and it certainly qualifies as one. However, the cloud-native concept does not exclude the use of on-premises infrastructure in cases where it makes sense. Depending on your organization\u2019s needs regarding compliance, locality and cost for running your workloads, there may be significant advantages to running Kubernetes deployments on-premises. Kubernetes has achieved an unprecedented adoption rate, due in part to the fact that it substantially simplifies the deployment and management of microservices. Almost equally important is that it allows users who are unable to utilize the public cloud to operate in a \u201ccloud-like\u201d environment. It does this by decoupling dependencies and abstracting infrastructure away from your application stack, giving you the portability and the scalability that\u2019s associated with cloud-native applications. Why Run Kubernetes On-premises \u00b6 Why do organizations choose the path of running Kubernetes in their own data centers, compared to the relative \u201ccake-walk\u201d with public cloud providers? There are typically a few important reasons why an Enterprise may choose to invest in a Kubernetes on-premises strategy: Compliance & Data Privacy Some organizations simply can\u2019t use the public cloud, as they are bound by stringent regulations related to compliance and data privacy issues. For example, the GDPR compliance rules may prevent enterprises from serving customers in the european region using services hosted in certain public clouds. Business Policy Reasons Business policy needs, such as having to run your workloads at specific geographical locations, may make it difficult to use public clouds. Some enterprises may not be able to utilize public cloud offerings from a specific cloud provider due to their business policies related to competition. Being Cloud Agnostic to Avoid Lock-in Many enterprises may wish to not be tried to a single cloud provider, and hence may want to deploy their applications across multiple clouds, including an on-premises private cloud. This reduces your risk of business continuity impact due to issues with a specific cloud provider. It also gives you / leverage around price negotiation with your cloud providers. Cost This is probably the most important reason to run Kubernetes on-premises. Running all of your applications in the public clouds can get pretty expensive at scale. Specifically if your applications rely on ingesting and processing a large amount of data , such as an AI/ML application, it can get extremely expensive to run it in a public cloud. If you have existing data centers on-premises or in a co-location hosted facility, running Kubernetes on-premises can be an effective way to reduce your operational costs. For more information on this topic, see this latest report from a16z: The Cost of Cloud, a Trillion Dollar Paradox Quoted from the report \u201d It\u2019s becoming evident that while cloud clearly delivers on its promise early on in a company\u2019s journey, the pressure it puts on margins can start to outweigh the benefits, as a company scales and growth slows. Because this shift happens later in a company\u2019s life, it is difficult to reverse as it\u2019s a result of years of development focused on new features, and not infrastructure optimization\u201d An effective strategy to run Kubernetes on-premises on your own data centers can be used to transform your business and modernize your applications for cloud-native \u2013 while improving infrastructure utilization and saving costs at the same time. Challenges Running Kubernetes On-premises \u00b6 There is a downside to running Kubernetes on-premises however, specially if you are going the Do-It-Yourself (DIY) route. Kubernetes is known for its steep learning curve and operational complexity. When using Kubernetes on AWS or Azure \u2013 your public cloud provider essentially hides all the complexities from you. Running Kubernetes on-premises means you\u2019re on your own for managing these complexities. Here are specific areas where the complexities are involved: Etcd \u2013 manage highly available etcd cluster. You need to take frequent backups to ensure business continuity in case the cluster goes down and the etcd data is lost. Load balancing \u2013 Load balancing may be needed both for your cluster master nodes and your application services running on Kubernetes. Depending on your existing networking setup, you may want to use a load balancer such as F5 or use a software load balancer such as metallb. Availability \u2013 Its critical to ensure that your Kubernetes infrastructure is highly available and can withstand data center and infrastructure downtimes. This would mean having multiple master nodes per cluster, and when relevant, having multiple Kubernetes clusters, across different availability zones. Auto-scaling \u2013 Auto-scaling for the nodes of your cluster can help save resources because the clusters can automatically expand and contract depending on workload needs. This is difficult to achieve for bare metal Kubernetes clusters, unless you are using a bare metal automation platform such as open source Ironic or Platform9\u2019s Managed Bare Metal. Networking \u2013 Networking is very specific to your data center configuration. Persistent storage \u2013 Majority of your production workloads running on Kubernetes will require persistent storage \u2013 block or file storage. The good news is that most of the popular enterprise storage vendors have CSI plugins and supported integrations with Kubernetes. You will need to work with your storage vendor to identify the right plugin and install any additional needed components before you can integration your existing storage solution with Kubernetes on-premises. Upgrades \u2013 You will need to upgrade your clusters roughly every 3 months when a new upstream version of Kubernetes is released. The version upgrade may create issues if there are API incompatibilities introduced with newer version of Kubernetes. A staged upgrading strategy where your development / test clusters are upgraded first before upgrading your production clusters is recommended. Monitoring \u2013 You will need to invest in tooling to monitor the health of your Kubernetes clusters in your on-premise Kubernetes environment. If you have existing monitoring and log management tools such as Datadog or Splunk, most of them have specific capabilities around Kubernetes monitoring. Or you may consider investing in an open source monitoring stack designed to help you monitor Kubernetes clusters such as Prometheus and Grafana. Best Practices for Kubernetes On-premises \u00b6 Below you will find a set of best practices to run Kubernetes on-premises. Depending on your environment configuration, some or all of these may apply to you. Integrating with Existing Environment Kubernetes enables users to run clusters on a diverse set of infrastructures on-premises . So you can \u201crepurpose\u201d your environment to integrate with Kubernetes \u2013 using virtual machines or creating your own cluster from scratch on bare metal. But you would need to build a deep understanding of the specifics of deploying Kubernetes on your existing environment, including your servers, storage systems, and networking infrastructure, to get a well configured production Kubernetes environment. The three most popular ways to deploy Kubernetes on-premises are: Virtual machines on your existing VMware vSphere environment Linux physical servers running Ubuntu, CentOS or RHEL Linux Virtual machines on other types of IaaS environments on-premises such as OpenStack. Running Kubernetes on physical servers can give you native hardware performance, which may be critical for certain types of workloads, however it may limit your ability to quickly scale your infrastructure. If getting bare metal performance is important to you, and if you need to run Kubernetes clusters at scale, then consider investing in a bare metal automation platform such as Ironic , Metal3 or a managed bare metal stack such as Platform9 Managed Bare Metal. Running Kubernetes on virtual machines in your private cloud on VMware or KVM can give you the elasticity of cloud, as you can dynamically scale your Kubernetes clusters up or down based on workload demand. Clusters created on virtual machines are also easy to setup and tear down, making it easy to create ephemeral test environments for developers.","title":"What is it?"},{"location":"accuknox-onprem/what-is-ak-on-prem/#overview","text":"The best Kubernetes architecture for your organization depends on your needs and goals. Kubernetes is often described as a cloud-native technology, and it certainly qualifies as one. However, the cloud-native concept does not exclude the use of on-premises infrastructure in cases where it makes sense. Depending on your organization\u2019s needs regarding compliance, locality and cost for running your workloads, there may be significant advantages to running Kubernetes deployments on-premises. Kubernetes has achieved an unprecedented adoption rate, due in part to the fact that it substantially simplifies the deployment and management of microservices. Almost equally important is that it allows users who are unable to utilize the public cloud to operate in a \u201ccloud-like\u201d environment. It does this by decoupling dependencies and abstracting infrastructure away from your application stack, giving you the portability and the scalability that\u2019s associated with cloud-native applications.","title":"Overview"},{"location":"accuknox-onprem/what-is-ak-on-prem/#why_run_kubernetes_on-premises","text":"Why do organizations choose the path of running Kubernetes in their own data centers, compared to the relative \u201ccake-walk\u201d with public cloud providers? There are typically a few important reasons why an Enterprise may choose to invest in a Kubernetes on-premises strategy: Compliance & Data Privacy Some organizations simply can\u2019t use the public cloud, as they are bound by stringent regulations related to compliance and data privacy issues. For example, the GDPR compliance rules may prevent enterprises from serving customers in the european region using services hosted in certain public clouds. Business Policy Reasons Business policy needs, such as having to run your workloads at specific geographical locations, may make it difficult to use public clouds. Some enterprises may not be able to utilize public cloud offerings from a specific cloud provider due to their business policies related to competition. Being Cloud Agnostic to Avoid Lock-in Many enterprises may wish to not be tried to a single cloud provider, and hence may want to deploy their applications across multiple clouds, including an on-premises private cloud. This reduces your risk of business continuity impact due to issues with a specific cloud provider. It also gives you / leverage around price negotiation with your cloud providers. Cost This is probably the most important reason to run Kubernetes on-premises. Running all of your applications in the public clouds can get pretty expensive at scale. Specifically if your applications rely on ingesting and processing a large amount of data , such as an AI/ML application, it can get extremely expensive to run it in a public cloud. If you have existing data centers on-premises or in a co-location hosted facility, running Kubernetes on-premises can be an effective way to reduce your operational costs. For more information on this topic, see this latest report from a16z: The Cost of Cloud, a Trillion Dollar Paradox Quoted from the report \u201d It\u2019s becoming evident that while cloud clearly delivers on its promise early on in a company\u2019s journey, the pressure it puts on margins can start to outweigh the benefits, as a company scales and growth slows. Because this shift happens later in a company\u2019s life, it is difficult to reverse as it\u2019s a result of years of development focused on new features, and not infrastructure optimization\u201d An effective strategy to run Kubernetes on-premises on your own data centers can be used to transform your business and modernize your applications for cloud-native \u2013 while improving infrastructure utilization and saving costs at the same time.","title":"Why Run Kubernetes On-premises"},{"location":"accuknox-onprem/what-is-ak-on-prem/#challenges_running_kubernetes_on-premises","text":"There is a downside to running Kubernetes on-premises however, specially if you are going the Do-It-Yourself (DIY) route. Kubernetes is known for its steep learning curve and operational complexity. When using Kubernetes on AWS or Azure \u2013 your public cloud provider essentially hides all the complexities from you. Running Kubernetes on-premises means you\u2019re on your own for managing these complexities. Here are specific areas where the complexities are involved: Etcd \u2013 manage highly available etcd cluster. You need to take frequent backups to ensure business continuity in case the cluster goes down and the etcd data is lost. Load balancing \u2013 Load balancing may be needed both for your cluster master nodes and your application services running on Kubernetes. Depending on your existing networking setup, you may want to use a load balancer such as F5 or use a software load balancer such as metallb. Availability \u2013 Its critical to ensure that your Kubernetes infrastructure is highly available and can withstand data center and infrastructure downtimes. This would mean having multiple master nodes per cluster, and when relevant, having multiple Kubernetes clusters, across different availability zones. Auto-scaling \u2013 Auto-scaling for the nodes of your cluster can help save resources because the clusters can automatically expand and contract depending on workload needs. This is difficult to achieve for bare metal Kubernetes clusters, unless you are using a bare metal automation platform such as open source Ironic or Platform9\u2019s Managed Bare Metal. Networking \u2013 Networking is very specific to your data center configuration. Persistent storage \u2013 Majority of your production workloads running on Kubernetes will require persistent storage \u2013 block or file storage. The good news is that most of the popular enterprise storage vendors have CSI plugins and supported integrations with Kubernetes. You will need to work with your storage vendor to identify the right plugin and install any additional needed components before you can integration your existing storage solution with Kubernetes on-premises. Upgrades \u2013 You will need to upgrade your clusters roughly every 3 months when a new upstream version of Kubernetes is released. The version upgrade may create issues if there are API incompatibilities introduced with newer version of Kubernetes. A staged upgrading strategy where your development / test clusters are upgraded first before upgrading your production clusters is recommended. Monitoring \u2013 You will need to invest in tooling to monitor the health of your Kubernetes clusters in your on-premise Kubernetes environment. If you have existing monitoring and log management tools such as Datadog or Splunk, most of them have specific capabilities around Kubernetes monitoring. Or you may consider investing in an open source monitoring stack designed to help you monitor Kubernetes clusters such as Prometheus and Grafana.","title":"Challenges Running Kubernetes On-premises"},{"location":"accuknox-onprem/what-is-ak-on-prem/#best_practices_for_kubernetes_on-premises","text":"Below you will find a set of best practices to run Kubernetes on-premises. Depending on your environment configuration, some or all of these may apply to you. Integrating with Existing Environment Kubernetes enables users to run clusters on a diverse set of infrastructures on-premises . So you can \u201crepurpose\u201d your environment to integrate with Kubernetes \u2013 using virtual machines or creating your own cluster from scratch on bare metal. But you would need to build a deep understanding of the specifics of deploying Kubernetes on your existing environment, including your servers, storage systems, and networking infrastructure, to get a well configured production Kubernetes environment. The three most popular ways to deploy Kubernetes on-premises are: Virtual machines on your existing VMware vSphere environment Linux physical servers running Ubuntu, CentOS or RHEL Linux Virtual machines on other types of IaaS environments on-premises such as OpenStack. Running Kubernetes on physical servers can give you native hardware performance, which may be critical for certain types of workloads, however it may limit your ability to quickly scale your infrastructure. If getting bare metal performance is important to you, and if you need to run Kubernetes clusters at scale, then consider investing in a bare metal automation platform such as Ironic , Metal3 or a managed bare metal stack such as Platform9 Managed Bare Metal. Running Kubernetes on virtual machines in your private cloud on VMware or KVM can give you the elasticity of cloud, as you can dynamically scale your Kubernetes clusters up or down based on workload demand. Clusters created on virtual machines are also easy to setup and tear down, making it easy to create ephemeral test environments for developers.","title":"Best Practices for Kubernetes On-premises"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/","text":"Auto Discovery is a policy recommendation system that suggests network and system policies based on the collected network and system logs respectively. Auto Discovery is designed for Kubernetes environments; it focuses on pods/services, and its fundamental principle is to produce a minimal network and system policy set covering maximum behavior. To do this, we actively use the label information assigned from the Kubernetes workloads/resources. Currently, Auto Discovery can discover (i) egress/ingress network policy for Pod-to- Pod, (External)Service, Entity, CIDR, FQDN, HTTP. And, In the System perspective it can discover (ii) process, file, and network-relevant system policy. Functionality Overview \u00b6 Produce a minimum network policy set covering maximum network flows When discovering the network policies, if we generate the policies applied to a single pod statically, there would be lots of network policies. In contrast, Auto Discovery produces the minimum network policy set that can cover the maximum network flows so that we can manage the network policies more efficiently and effectively. For example, Auto Discovery collects the label information of the pods, and then computes the intersection of labels, which is the most included in the source (or destination) pods. Identify overlapped network policy Regarding the external destination, Auto Discovery builds CIDR or FQDN-based policies, and to do this it takes two steps. First, if it comes across the external IP address as the destination, it tries to convert the IP address to the domain name by leveraging the reverse domain services. Next, if it fails to find the domain name, it retrieves the domain name from an internal map that matches the domain name to the IP address collected by DNS query and response packets from the kube-dns traffic. Thus, building FQDN based policies has a higher priority than CIDR policies. Inevitably, CIDR policies could be discovered if there is no information on the matched domain names. However, if we build an FQDN policy that overlaps the prior CIDR policy, Auto Discovery can tag and update those policies so that we can maintain the latest network policies. Operate in runtime or on the collected network logs in advance Generally, Auto Discovery discovers the network policies by extracting the network logs from the database every time intervals. In addition, It can connect to a log monitor directly (e.g., Cilium Hubble), and receive the network log, and then produce the network policies in runtime. Support various network and system policy discovery modes Fundamentally, a pod has two types of network policy in Kubernetes; egress and ingress. The egress policy restricts the outbound network flows and the other way, the ingress policy operates against the inbound network flows. In this context, Auto Discovery supports both types of policy discovery modes; egress-centric and ingress-centric. Additionally System-centric also. System Policy can be of process, file, and network types. Thus, users can choose one of them depending on their demand. Policy Discovery Examples \u00b6 The intersection of matched labels \u00b6 Let's assume that there are three pods and two connections between them as shown in the above figure. From the network logs from it, the Auto Discovery does not discover the two distinct network policies; [Pod A -> Pod C] and [Pod A -> Pod C]. Instead, since Pod A and Pod B have the intersection of the labels, which is 'group=alice', the knoxAutoPolicy discovers and generates a network policy that has the selector with matchLabels 'group=alice'. Finally, we can get one network policy that covers two distinct network flows [group=alice -> Pod C]. The aggregation of toPorts rules per the same destination \u00b6 Similar to the previous case, we can merge the multiple toPorts rules per each same destination. Let's assume that there are the source and destination pods and three different flows as shown in the above figure. In this case, the Auto Discovery does not generate three different network policies per each toPorts rule. More efficiently, the Auto Discovery discovers one network policy and has one matchLabel rule and three toPorts rules; port numbers are 443, 8080, and 80. From this merge, it can be enabled to produce a minimum network policy set covering maximum network flows. The trace of the outdated network policy \u00b6 Since the Auto Discovery can do the discovery job at the time intervals, there could be some overlapping. For example, as shown in the above figure, let's assume we discovered policy A and B at the time t1 and t2 respectively. However, policy B has the same toCIDRs rule as policy A does but a different toPorts rule. In this case, It updates policy B by merging the toPorts rule to the latest one, and then it marks policy A as outdated and puts the relevant latest policy name. Thus, users can retrieve only the latest network policies from the database. Auto Discovered Policies Dashboard \u00b6 You can filter Auto Discovered Policies using following filters: Cluster :- Filter Policies by clusters belonging to your workspace. Namespace: Filter Policies by namespaces belonging to selected clusters Policy-Type: Filter Policies by Policy types. There are 3 Policy types. (i) Network-Ingress (ii) Network-Egress (iii) System Policy Category: Category will give the status of the policies. There are 2 categories. Used : When the Policy is applied from the Auto Discovered Policy Screen, It will go to All Policy Screen and Category will be changed to Used . You can list all used policies with used category. Ignore: You can list all ignored policies using this filter. Apply Auto Discovered Policies. \u00b6 Select one or more policies from the list Note: Default screen will show all un-used policies. Click \u201c Action \u201d Button on the top right corner. There are 3 Actions can be performed. (i) Apply (ii) Ignore (iii) Deselect all Click Apply . Then Policy will be applied to the cluster. Applied Policy will go to pending approval. Go to \u201cPending Approval\u201d screen and Approve the policy. Note: You need Administrative permission to approve policies. Approved Policy will go to All Policies Screen.","title":"Auto Discovery of Policies"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/#functionality_overview","text":"Produce a minimum network policy set covering maximum network flows When discovering the network policies, if we generate the policies applied to a single pod statically, there would be lots of network policies. In contrast, Auto Discovery produces the minimum network policy set that can cover the maximum network flows so that we can manage the network policies more efficiently and effectively. For example, Auto Discovery collects the label information of the pods, and then computes the intersection of labels, which is the most included in the source (or destination) pods. Identify overlapped network policy Regarding the external destination, Auto Discovery builds CIDR or FQDN-based policies, and to do this it takes two steps. First, if it comes across the external IP address as the destination, it tries to convert the IP address to the domain name by leveraging the reverse domain services. Next, if it fails to find the domain name, it retrieves the domain name from an internal map that matches the domain name to the IP address collected by DNS query and response packets from the kube-dns traffic. Thus, building FQDN based policies has a higher priority than CIDR policies. Inevitably, CIDR policies could be discovered if there is no information on the matched domain names. However, if we build an FQDN policy that overlaps the prior CIDR policy, Auto Discovery can tag and update those policies so that we can maintain the latest network policies. Operate in runtime or on the collected network logs in advance Generally, Auto Discovery discovers the network policies by extracting the network logs from the database every time intervals. In addition, It can connect to a log monitor directly (e.g., Cilium Hubble), and receive the network log, and then produce the network policies in runtime. Support various network and system policy discovery modes Fundamentally, a pod has two types of network policy in Kubernetes; egress and ingress. The egress policy restricts the outbound network flows and the other way, the ingress policy operates against the inbound network flows. In this context, Auto Discovery supports both types of policy discovery modes; egress-centric and ingress-centric. Additionally System-centric also. System Policy can be of process, file, and network types. Thus, users can choose one of them depending on their demand.","title":"Functionality Overview"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/#policy_discovery_examples","text":"","title":"Policy Discovery Examples"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/#the_intersection_of_matched_labels","text":"Let's assume that there are three pods and two connections between them as shown in the above figure. From the network logs from it, the Auto Discovery does not discover the two distinct network policies; [Pod A -> Pod C] and [Pod A -> Pod C]. Instead, since Pod A and Pod B have the intersection of the labels, which is 'group=alice', the knoxAutoPolicy discovers and generates a network policy that has the selector with matchLabels 'group=alice'. Finally, we can get one network policy that covers two distinct network flows [group=alice -> Pod C].","title":"The intersection of matched labels"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/#the_aggregation_of_toports_rules_per_the_same_destination","text":"Similar to the previous case, we can merge the multiple toPorts rules per each same destination. Let's assume that there are the source and destination pods and three different flows as shown in the above figure. In this case, the Auto Discovery does not generate three different network policies per each toPorts rule. More efficiently, the Auto Discovery discovers one network policy and has one matchLabel rule and three toPorts rules; port numbers are 443, 8080, and 80. From this merge, it can be enabled to produce a minimum network policy set covering maximum network flows.","title":"The aggregation of toPorts rules per the same destination"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/#the_trace_of_the_outdated_network_policy","text":"Since the Auto Discovery can do the discovery job at the time intervals, there could be some overlapping. For example, as shown in the above figure, let's assume we discovered policy A and B at the time t1 and t2 respectively. However, policy B has the same toCIDRs rule as policy A does but a different toPorts rule. In this case, It updates policy B by merging the toPorts rule to the latest one, and then it marks policy A as outdated and puts the relevant latest policy name. Thus, users can retrieve only the latest network policies from the database.","title":"The trace of the outdated network policy"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/#auto_discovered_policies_dashboard","text":"You can filter Auto Discovered Policies using following filters: Cluster :- Filter Policies by clusters belonging to your workspace. Namespace: Filter Policies by namespaces belonging to selected clusters Policy-Type: Filter Policies by Policy types. There are 3 Policy types. (i) Network-Ingress (ii) Network-Egress (iii) System Policy Category: Category will give the status of the policies. There are 2 categories. Used : When the Policy is applied from the Auto Discovered Policy Screen, It will go to All Policy Screen and Category will be changed to Used . You can list all used policies with used category. Ignore: You can list all ignored policies using this filter.","title":"Auto Discovered Policies Dashboard"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/#apply_auto_discovered_policies","text":"Select one or more policies from the list Note: Default screen will show all un-used policies. Click \u201c Action \u201d Button on the top right corner. There are 3 Actions can be performed. (i) Apply (ii) Ignore (iii) Deselect all Click Apply . Then Policy will be applied to the cluster. Applied Policy will go to pending approval. Go to \u201cPending Approval\u201d screen and Approve the policy. Note: You need Administrative permission to approve policies. Approved Policy will go to All Policies Screen.","title":"Apply Auto Discovered Policies."},{"location":"extensions/code-hilite/","text":"CodeHilite \u00b6 CodeHilite - Material for MkDocs Supported languages - Pygments Configure mkdocs.yml \u00b6 markdown_extensions: - codehilite","title":"CodeHilite"},{"location":"extensions/code-hilite/#codehilite","text":"CodeHilite - Material for MkDocs Supported languages - Pygments","title":"CodeHilite"},{"location":"extensions/code-hilite/#configure_mkdocsyml","text":"markdown_extensions: - codehilite","title":"Configure mkdocs.yml"},{"location":"extensions/footnote/","text":"Footnote \u00b6 Footnotes - Material for MkDocs Configure mkdocs.yml \u00b6 markdown_extensions: - footnotes Example \u00b6 Footnote example 1. 1 Footnote example 2. 2 One line \u21a9 First line Second line \u21a9","title":"Footnote"},{"location":"extensions/footnote/#footnote","text":"Footnotes - Material for MkDocs","title":"Footnote"},{"location":"extensions/footnote/#configure_mkdocsyml","text":"markdown_extensions: - footnotes","title":"Configure mkdocs.yml"},{"location":"extensions/footnote/#example","text":"Footnote example 1. 1 Footnote example 2. 2 One line \u21a9 First line Second line \u21a9","title":"Example"},{"location":"extensions/mathjax/","text":"MathJax \u00b6 PyMdown - Material for MkDocs Configure mkdocs.yml \u00b6 markdown_extensions: - mdx_math: enable_dollar_delimiter: True Example code \u00b6 $$ P \\c dot Q = \\| P \\|\\| Q \\|\\c os \\a lpha $$ Example rendering \u00b6 P\\cdot Q = \\|P\\|\\|Q\\|\\cos\\alpha","title":"MathJax"},{"location":"extensions/mathjax/#mathjax","text":"PyMdown - Material for MkDocs","title":"MathJax"},{"location":"extensions/mathjax/#configure_mkdocsyml","text":"markdown_extensions: - mdx_math: enable_dollar_delimiter: True","title":"Configure mkdocs.yml"},{"location":"extensions/mathjax/#example_code","text":"$$ P \\c dot Q = \\| P \\|\\| Q \\|\\c os \\a lpha $$","title":"Example code"},{"location":"extensions/mathjax/#example_rendering","text":"P\\cdot Q = \\|P\\|\\|Q\\|\\cos\\alpha","title":"Example rendering"},{"location":"getting-started/Logs/","text":"Logs and Triggers \u00b6 1. What Kind of Logs: \u00b6 The logs generated from each component like Network / System / Data Protection / Anamoly Detection can be viewed under Logs section. The logs can be viewed for independent workloads like K8's or VM's Each of the component should be up and Running in the installed cluster for the logs to be seen in the Workspace . NOTE: It's assumed that All Agents is running on cluster if not kindly go through this section 2. Triggers: \u00b6 An Alert trigger is a rule which triggers information transfer through a known channel. Customer can send information [log] in Real Time when it occurs or on specified Frequency. Frequency is configurable which can be Once In a Day/ Week / Month. Predefined Filters (or) pre-saved global filters are search queries on logs that is already available in the system. Example : show logs for traffic direction = \"EGRESS\" 3. Configuration of Alert Triggers: \u00b6 On the Logs page, after choosing specific log filter click on 'Create Trigger' button. The below fields needs to be entered with appropriate data: Name: Enter the name for the trigger. You can set any name without special characters. When to Initiate: The frequency of the trigger as Real Time / . Status: Enter the severity for the trigger. Search Filter Data : The filter log chosen in automatically populated here. This is optional. Predefined queries: The list of predefined queries for this workspace is shown as default. Notification Channel: Select the integration channel that needs to receive logs. Can be Slack/ Splunk / Cloudwatch / Elastic. (Note: Channel Integrations needs to be done beforehand) Save: Click on Save for the trigger to get stored in database. a. List Of Triggers: \u00b6 Select Triggers from the left navigation to view the list of triggers created. (If Default params needs to be modified) The below fields for each trigger can be viewed. Alert Trigger : Name of the Trigger Created At : Time of Trigger Creation Channel : Integration Channel Name Enable/Disable: Enabling / Disabling the trigger to forward the logs into Integration Channel. Details : The filter query of the associated Trigger. b. Actions: \u00b6 Select the below Actions on the triggers to check working of logs forwarding. (If Default params needs to be modified) The below actions for each trigger can be selected. Enable : Enabling the trigger to forward the logs into Integration Channel. Disable : Disabling the trigger to not forward the logs into Integration Channel. Delete : Delete the Logs Trigger Export : Exporting the triggers in PDF format. 4. Logs Forwarding: \u00b6 For each Enabled Trigger, please check the integrated channel to view the logs. The Rule Engine matches the real time logs against the triggers created. Frequency >> Real Time The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel as it happens. Frequency >> Once in A Day The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel only Once in a Day. Frequency >> Once in A Month The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel only Once in a Month. Frequency >> Once in A Week The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel only Once in a Week. NOTE: Frequency reduces channel noise / and safely provides only one informational log on the channels based on frequency time period.","title":"Logs and Triggers"},{"location":"getting-started/Logs/#logs_and_triggers","text":"","title":"Logs and Triggers"},{"location":"getting-started/Logs/#1_what_kind_of_logs","text":"The logs generated from each component like Network / System / Data Protection / Anamoly Detection can be viewed under Logs section. The logs can be viewed for independent workloads like K8's or VM's Each of the component should be up and Running in the installed cluster for the logs to be seen in the Workspace . NOTE: It's assumed that All Agents is running on cluster if not kindly go through this section","title":"1. What Kind of Logs:"},{"location":"getting-started/Logs/#2_triggers","text":"An Alert trigger is a rule which triggers information transfer through a known channel. Customer can send information [log] in Real Time when it occurs or on specified Frequency. Frequency is configurable which can be Once In a Day/ Week / Month. Predefined Filters (or) pre-saved global filters are search queries on logs that is already available in the system. Example : show logs for traffic direction = \"EGRESS\"","title":"2. Triggers:"},{"location":"getting-started/Logs/#3_configuration_of_alert_triggers","text":"On the Logs page, after choosing specific log filter click on 'Create Trigger' button. The below fields needs to be entered with appropriate data: Name: Enter the name for the trigger. You can set any name without special characters. When to Initiate: The frequency of the trigger as Real Time / . Status: Enter the severity for the trigger. Search Filter Data : The filter log chosen in automatically populated here. This is optional. Predefined queries: The list of predefined queries for this workspace is shown as default. Notification Channel: Select the integration channel that needs to receive logs. Can be Slack/ Splunk / Cloudwatch / Elastic. (Note: Channel Integrations needs to be done beforehand) Save: Click on Save for the trigger to get stored in database.","title":"3. Configuration of Alert Triggers:"},{"location":"getting-started/Logs/#a_list_of_triggers","text":"Select Triggers from the left navigation to view the list of triggers created. (If Default params needs to be modified) The below fields for each trigger can be viewed. Alert Trigger : Name of the Trigger Created At : Time of Trigger Creation Channel : Integration Channel Name Enable/Disable: Enabling / Disabling the trigger to forward the logs into Integration Channel. Details : The filter query of the associated Trigger.","title":"a. List Of Triggers:"},{"location":"getting-started/Logs/#b_actions","text":"Select the below Actions on the triggers to check working of logs forwarding. (If Default params needs to be modified) The below actions for each trigger can be selected. Enable : Enabling the trigger to forward the logs into Integration Channel. Disable : Disabling the trigger to not forward the logs into Integration Channel. Delete : Delete the Logs Trigger Export : Exporting the triggers in PDF format.","title":"b. Actions:"},{"location":"getting-started/Logs/#4_logs_forwarding","text":"For each Enabled Trigger, please check the integrated channel to view the logs. The Rule Engine matches the real time logs against the triggers created. Frequency >> Real Time The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel as it happens. Frequency >> Once in A Day The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel only Once in a Day. Frequency >> Once in A Month The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel only Once in a Month. Frequency >> Once in A Week The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel only Once in a Week. NOTE: Frequency reduces channel noise / and safely provides only one informational log on the channels based on frequency time period.","title":"4. Logs Forwarding:"},{"location":"getting-started/agent-metrics/","text":"Agent Metrics: FileBeat | Kibana | On-prem Metrics \u00b6 1. Status of Feeder Agent running on Cluster: \u00b6 Please run the below command to check if agent and dependent pods are up and running. kubectl get all \u2013n feeder-service All the pods/services should be in Running state. NOTE: It's assumed that Feeder Agent is running on cluster if not kindly go through this section 2. Beats Setup: \u00b6 The agent will be spinned along with Filebeat running along as a sidecar. The filebeat configuration file in the package can be updated to specific Elastic instances, and logs can be viewed in Kibana . a. Elastic Configuration Parameters: \u00b6 The below Configuration parameters can be updated for elastic configuration. (If Default params needs to be modified) - name : ELASTICSEARCH_HOST value : https://elasticsearch-es-http - name : ELASTICSEARCH_PORT value : \"9200\" - name : ELASTICSEARCH_USERNAME value : \"elastic\" - name : ELASTICSEARCH_PASSWORD value : \"xxxxxxxxxxxxx\" b. Command to be Used: \u00b6 kubectl set env deploy/feeder -n feeder-service ELASTICSEARCH_HOST = \u201dhttps://elasticsearch-es-http\u201d c. Update Log Path: \u00b6 To Update the Log path configured, please modify the below log input path under file beat inputs. filebeat.inputs : - type : container paths : - /log_output/value.log 2. Kibana Dashboard \u00b6 Once the filebeat starts listening, an index will be created or updated on the elastic configured and the pushed logs can be seen. In order to create a dashboard, you will need to build visualizations. Kibana has two panels for this One called Visualize and Another called Dashboard In order to create your dashboard, you will first create every individual visualization with the Visualize panel and save them. 3. Metrics: \u00b6 Once the feeder agent starts running, check the logs using below command Kubectl logs \u2013f podname \u2013n feeder-service The logs will push the metric data to GRPC Client / Kafka, and the GRPC server in SaaS platform will be listening to this metrics and can be viewed in Prometheus. (prometheus-dev.accuknox.com) 4. On Prem Metrics: \u00b6 To fetch the metrics in standalone environment, please write a scrape job in Prometheus with feeder service agent as a job name, and scrape the metrics from port (:xxxx) - job_name : \"feeder-pod-agent\" sample_limit : 10000","title":"Agent Metrics"},{"location":"getting-started/agent-metrics/#agent_metrics_filebeat_kibana_on-prem_metrics","text":"","title":"Agent Metrics: FileBeat | Kibana | On-prem Metrics"},{"location":"getting-started/agent-metrics/#1_status_of_feeder_agent_running_on_cluster","text":"Please run the below command to check if agent and dependent pods are up and running. kubectl get all \u2013n feeder-service All the pods/services should be in Running state. NOTE: It's assumed that Feeder Agent is running on cluster if not kindly go through this section","title":"1. Status of Feeder Agent running on Cluster:"},{"location":"getting-started/agent-metrics/#2_beats_setup","text":"The agent will be spinned along with Filebeat running along as a sidecar. The filebeat configuration file in the package can be updated to specific Elastic instances, and logs can be viewed in Kibana .","title":"2. Beats Setup:"},{"location":"getting-started/agent-metrics/#a_elastic_configuration_parameters","text":"The below Configuration parameters can be updated for elastic configuration. (If Default params needs to be modified) - name : ELASTICSEARCH_HOST value : https://elasticsearch-es-http - name : ELASTICSEARCH_PORT value : \"9200\" - name : ELASTICSEARCH_USERNAME value : \"elastic\" - name : ELASTICSEARCH_PASSWORD value : \"xxxxxxxxxxxxx\"","title":"a. Elastic Configuration Parameters:"},{"location":"getting-started/agent-metrics/#b_command_to_be_used","text":"kubectl set env deploy/feeder -n feeder-service ELASTICSEARCH_HOST = \u201dhttps://elasticsearch-es-http\u201d","title":"b. Command to be Used:"},{"location":"getting-started/agent-metrics/#c_update_log_path","text":"To Update the Log path configured, please modify the below log input path under file beat inputs. filebeat.inputs : - type : container paths : - /log_output/value.log","title":"c. Update Log Path:"},{"location":"getting-started/agent-metrics/#2_kibana_dashboard","text":"Once the filebeat starts listening, an index will be created or updated on the elastic configured and the pushed logs can be seen. In order to create a dashboard, you will need to build visualizations. Kibana has two panels for this One called Visualize and Another called Dashboard In order to create your dashboard, you will first create every individual visualization with the Visualize panel and save them.","title":"2. Kibana Dashboard"},{"location":"getting-started/agent-metrics/#3_metrics","text":"Once the feeder agent starts running, check the logs using below command Kubectl logs \u2013f podname \u2013n feeder-service The logs will push the metric data to GRPC Client / Kafka, and the GRPC server in SaaS platform will be listening to this metrics and can be viewed in Prometheus. (prometheus-dev.accuknox.com)","title":"3. Metrics:"},{"location":"getting-started/agent-metrics/#4_on_prem_metrics","text":"To fetch the metrics in standalone environment, please write a scrape job in Prometheus with feeder service agent as a job name, and scrape the metrics from port (:xxxx) - job_name : \"feeder-pod-agent\" sample_limit : 10000","title":"4. On Prem Metrics:"},{"location":"getting-started/channel-integration/","text":"Channel Integration \u00b6 Channel Integrations is the fourth sub-section of Workspace Manager. This section is used to integrate external services with AccuKnox so you can export logs as well as metrics. These services include: Webhooks Slack Jira Elastic Search Pagerduty Syslog Splunk Email ServiceNow AWS CloudWatch Choose any one of the services and click the Integrate Now button. 1. Integration of Slack: \u00b6 a. Prerequisites: \u00b6 You need a valid and active account in Slack. After logging into your Slack channel, you must generate a Hook URL. [Note]: If you don\u2019t know how to get Hook URL then click this link and follow the steps. b. Steps to Integrate: \u00b6 Goto Channel Integration URL Click the Integrate Now button inside Slack Here you'll be able to see these entries: Integration Name: Enter the name for the integration. You can set any name. Hook URL: Enter your slack hook URL here. Sender Name: Enter the sender name here. Channel Name: Enter your slack channel name here. Message Title: You can set a message title using this input field. This is optional. Tags to be sent with alerts: You can set tags using this input field. This is optional. Once you fill every field then click the button this will test whether your integration is working or not. Click the Save button. 2. Integration of Splunk: \u00b6 a. Prerequisites \u00b6 You need a Splunk HTTP event collector URL for this Integration. [Note]: If you don\u2019t know how to get Splunk HTTP event collector URL then click this link b. Steps to Integrate: \u00b6 Goto Channel Integration URL Click the Integrate Now button inside Splunk Here you'll be able to see these entries: Integration Name: Enter the name for the integration. You can set any name. Splunk HTTP event collector URL: Enter your Splunk HTTP event collector URL here. Token: Enter your Splunk Token here. Source: Enter your Splunk channel name here. Index: Enter your Splunk Index here. Source Type: Enter your Source Type here. Enable HTTPS: If you want HTTPS service then enable this button. Enable TLS Verify: If you want TLS service then enable this button. Once you fill every field then click the button this will test whether your integration is working or not. Click the Save button.","title":"What is Channel Integration"},{"location":"getting-started/channel-integration/#channel_integration","text":"Channel Integrations is the fourth sub-section of Workspace Manager. This section is used to integrate external services with AccuKnox so you can export logs as well as metrics. These services include: Webhooks Slack Jira Elastic Search Pagerduty Syslog Splunk Email ServiceNow AWS CloudWatch Choose any one of the services and click the Integrate Now button.","title":"Channel Integration"},{"location":"getting-started/channel-integration/#1_integration_of_slack","text":"","title":"1. Integration of Slack:"},{"location":"getting-started/channel-integration/#a_prerequisites","text":"You need a valid and active account in Slack. After logging into your Slack channel, you must generate a Hook URL. [Note]: If you don\u2019t know how to get Hook URL then click this link and follow the steps.","title":"a. Prerequisites:"},{"location":"getting-started/channel-integration/#b_steps_to_integrate","text":"Goto Channel Integration URL Click the Integrate Now button inside Slack Here you'll be able to see these entries: Integration Name: Enter the name for the integration. You can set any name. Hook URL: Enter your slack hook URL here. Sender Name: Enter the sender name here. Channel Name: Enter your slack channel name here. Message Title: You can set a message title using this input field. This is optional. Tags to be sent with alerts: You can set tags using this input field. This is optional. Once you fill every field then click the button this will test whether your integration is working or not. Click the Save button.","title":"b. Steps to Integrate:"},{"location":"getting-started/channel-integration/#2_integration_of_splunk","text":"","title":"2. Integration of Splunk:"},{"location":"getting-started/channel-integration/#a_prerequisites_1","text":"You need a Splunk HTTP event collector URL for this Integration. [Note]: If you don\u2019t know how to get Splunk HTTP event collector URL then click this link","title":"a. Prerequisites"},{"location":"getting-started/channel-integration/#b_steps_to_integrate_1","text":"Goto Channel Integration URL Click the Integrate Now button inside Splunk Here you'll be able to see these entries: Integration Name: Enter the name for the integration. You can set any name. Splunk HTTP event collector URL: Enter your Splunk HTTP event collector URL here. Token: Enter your Splunk Token here. Source: Enter your Splunk channel name here. Index: Enter your Splunk Index here. Source Type: Enter your Source Type here. Enable HTTPS: If you want HTTPS service then enable this button. Enable TLS Verify: If you want TLS service then enable this button. Once you fill every field then click the button this will test whether your integration is working or not. Click the Save button.","title":"b. Steps to Integrate:"},{"location":"getting-started/cilium-install/","text":"Cilium: Deployment Guide \u00b6 Deployment Steps for Cilium & Hubble CLI \u00b6 1. Download and install Cilium CLI \u00b6 curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } 2. Install Cilium \u00b6 cilium install It is assumed that the k8s cluster is already present/reachable and the user has rights to create service-accounts and cluster-role-bindings. 3. Validate the Installation \u00b6 a. [Optional] To validate that Cilium has been properly installed, you can run: \u00b6 cilium status --wait b. [Optional] Run the following command to validate that your cluster has proper network connectivity: \u00b6 cilium connectivity test Congratulations! You have a fully functional Kubernetes cluster with Cilium. \ud83c\udf89 4. Setting up Hubble Observability \u00b6 a. Enable Hubble in Cilium \u00b6 cilium hubble enable b. Install the Hubble CLI Client \u00b6 export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum } 5. Getting Alerts/Telemetry from Cilium \u00b6 a. Enable port-forwarding for Cilium Hubble relay \u00b6 cilium hubble port-forward & b. Observing logs using hubble cli \u00b6 hubble observe","title":"Cilium: Deployment Guide"},{"location":"getting-started/cilium-install/#cilium_deployment_guide","text":"","title":"Cilium: Deployment Guide"},{"location":"getting-started/cilium-install/#deployment_steps_for_cilium_hubble_cli","text":"","title":"Deployment Steps for Cilium &amp; Hubble CLI"},{"location":"getting-started/cilium-install/#1_download_and_install_cilium_cli","text":"curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum }","title":"1. Download and install Cilium CLI"},{"location":"getting-started/cilium-install/#2_install_cilium","text":"cilium install It is assumed that the k8s cluster is already present/reachable and the user has rights to create service-accounts and cluster-role-bindings.","title":"2. Install Cilium"},{"location":"getting-started/cilium-install/#3_validate_the_installation","text":"","title":"3. Validate the Installation"},{"location":"getting-started/cilium-install/#a_optional_to_validate_that_cilium_has_been_properly_installed_you_can_run","text":"cilium status --wait","title":"a. [Optional] To validate that Cilium has been properly installed, you can run:"},{"location":"getting-started/cilium-install/#b_optional_run_the_following_command_to_validate_that_your_cluster_has_proper_network_connectivity","text":"cilium connectivity test Congratulations! You have a fully functional Kubernetes cluster with Cilium. \ud83c\udf89","title":"b. [Optional] Run the following command to validate that your cluster has proper network connectivity:"},{"location":"getting-started/cilium-install/#4_setting_up_hubble_observability","text":"","title":"4. Setting up Hubble Observability"},{"location":"getting-started/cilium-install/#a_enable_hubble_in_cilium","text":"cilium hubble enable","title":"a. Enable Hubble in Cilium"},{"location":"getting-started/cilium-install/#b_install_the_hubble_cli_client","text":"export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum }","title":"b. Install the Hubble CLI Client"},{"location":"getting-started/cilium-install/#5_getting_alertstelemetry_from_cilium","text":"","title":"5. Getting Alerts/Telemetry from Cilium"},{"location":"getting-started/cilium-install/#a_enable_port-forwarding_for_cilium_hubble_relay","text":"cilium hubble port-forward &","title":"a. Enable port-forwarding for Cilium Hubble relay"},{"location":"getting-started/cilium-install/#b_observing_logs_using_hubble_cli","text":"hubble observe","title":"b. Observing logs using hubble cli"},{"location":"getting-started/docker/","text":"Start with Docker \u00b6 Public docker image \u00b6 Package peaceiris/mkdocs-material docker-compose \u00b6 Here is an example docker-compose.yml Please check the latest tag before you go. docker-compose up Go to http://localhost:8000/","title":"Start with Docker"},{"location":"getting-started/docker/#start_with_docker","text":"","title":"Start with Docker"},{"location":"getting-started/docker/#public_docker_image","text":"Package peaceiris/mkdocs-material","title":"Public docker image"},{"location":"getting-started/docker/#docker-compose","text":"Here is an example docker-compose.yml Please check the latest tag before you go. docker-compose up Go to http://localhost:8000/","title":"docker-compose"},{"location":"getting-started/workspace-manager/","text":"Workspace Manager in AccuKnox \u00b6 In Workspace Manager, user can perform various tasks such as managing users and roles, onboard cluster and adding third-party integrations etc. The workspace manager has 4 subsections. These four sections are: User Management Role-Based Access Control Onboard Cluster Channel Integrations","title":"What is Workspace Manager"},{"location":"getting-started/workspace-manager/#workspace_manager_in_accuknox","text":"In Workspace Manager, user can perform various tasks such as managing users and roles, onboard cluster and adding third-party integrations etc. The workspace manager has 4 subsections. These four sections are: User Management Role-Based Access Control Onboard Cluster Channel Integrations","title":"Workspace Manager in AccuKnox"},{"location":"open-source/cilium-install/","text":"Cilium: Deployment Guide \u00b6 Deployment Steps for Cilium & Hubble CLI \u00b6 1. Download and install Cilium CLI \u00b6 curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } 2. Install Cilium \u00b6 cilium install It is assumed that the k8s cluster is already present/reachable and the user has rights to create service-accounts and cluster-role-bindings. 3. Validate the Installation \u00b6 a. [Optional] To validate that Cilium has been properly installed, you can run: \u00b6 cilium status --wait b. [Optional] Run the following command to validate that your cluster has proper network connectivity: \u00b6 cilium connectivity test Congratulations! You have a fully functional Kubernetes cluster with Cilium. \ud83c\udf89 4. Setting up Hubble Observability \u00b6 a. Enable Hubble in Cilium \u00b6 cilium hubble enable b. Install the Hubble CLI Client \u00b6 export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum } 5. Getting Alerts/Telemetry from Cilium \u00b6 a. Enable port-forwarding for Cilium Hubble relay \u00b6 cilium hubble port-forward & b. Observing logs using hubble cli \u00b6 hubble observe","title":"Installing Cilium"},{"location":"open-source/cilium-install/#cilium_deployment_guide","text":"","title":"Cilium: Deployment Guide"},{"location":"open-source/cilium-install/#deployment_steps_for_cilium_hubble_cli","text":"","title":"Deployment Steps for Cilium &amp; Hubble CLI"},{"location":"open-source/cilium-install/#1_download_and_install_cilium_cli","text":"curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum }","title":"1. Download and install Cilium CLI"},{"location":"open-source/cilium-install/#2_install_cilium","text":"cilium install It is assumed that the k8s cluster is already present/reachable and the user has rights to create service-accounts and cluster-role-bindings.","title":"2. Install Cilium"},{"location":"open-source/cilium-install/#3_validate_the_installation","text":"","title":"3. Validate the Installation"},{"location":"open-source/cilium-install/#a_optional_to_validate_that_cilium_has_been_properly_installed_you_can_run","text":"cilium status --wait","title":"a. [Optional] To validate that Cilium has been properly installed, you can run:"},{"location":"open-source/cilium-install/#b_optional_run_the_following_command_to_validate_that_your_cluster_has_proper_network_connectivity","text":"cilium connectivity test Congratulations! You have a fully functional Kubernetes cluster with Cilium. \ud83c\udf89","title":"b. [Optional] Run the following command to validate that your cluster has proper network connectivity:"},{"location":"open-source/cilium-install/#4_setting_up_hubble_observability","text":"","title":"4. Setting up Hubble Observability"},{"location":"open-source/cilium-install/#a_enable_hubble_in_cilium","text":"cilium hubble enable","title":"a. Enable Hubble in Cilium"},{"location":"open-source/cilium-install/#b_install_the_hubble_cli_client","text":"export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum }","title":"b. Install the Hubble CLI Client"},{"location":"open-source/cilium-install/#5_getting_alertstelemetry_from_cilium","text":"","title":"5. Getting Alerts/Telemetry from Cilium"},{"location":"open-source/cilium-install/#a_enable_port-forwarding_for_cilium_hubble_relay","text":"cilium hubble port-forward &","title":"a. Enable port-forwarding for Cilium Hubble relay"},{"location":"open-source/cilium-install/#b_observing_logs_using_hubble_cli","text":"hubble observe","title":"b. Observing logs using hubble cli"},{"location":"open-source/kubearmor-install/","text":"KubeArmor: Deployment Guide \u00b6 Deployment Steps for KubeArmor & kArmor CLI \u00b6 1. Download and install karmor CLI \u00b6 curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin 2. Install KubeArmor \u00b6 karmor install It is assumed that the k8s cluster is already present/reachable and the user has rights to create service-accounts and cluster-role-bindings. 3. Deploying sample app and policies \u00b6 a. Deploy sample multiubuntu app \u00b6 kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/master/examples/multiubuntu/multiubuntu-deployment.yaml b. Deploy sample policies \u00b6 kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/master/examples/multiubuntu/security-policies/ksp-group-1-proc-path-block.yaml This sample policy blocks execution of sleep command in ubuntu-1 pods. c. Simulate policy violation \u00b6 $ kubectl -n multiubuntu exec -it POD_NAME_FOR_UBUNTU_1 -- bash # sleep 1 ( Permission Denied ) Substitute POD_NAME_FOR_UBUNTU_1 with the actual pod name from kubectl get pods -n multiubuntu . 4. Getting Alerts/Telemetry from KubeArmor \u00b6 a. Enable port-forwarding for KubeArmor relay \u00b6 kubectl port-forward -n kube-system svc/kubearmor 32767 :32767 b. Observing logs using karmor cli \u00b6 karmor log K8s platforms tested \u00b6 Google Kubernetes Engine (GKE) Container Optimized OS (COS) GKE Ubuntu image Amazon Elastic Kubernetes Service (EKS) Self-managed (on-prem) k8s Local k8s engines (microk8s, k3s, minikube)","title":"Installing KubeArmor"},{"location":"open-source/kubearmor-install/#kubearmor_deployment_guide","text":"","title":"KubeArmor: Deployment Guide"},{"location":"open-source/kubearmor-install/#deployment_steps_for_kubearmor_karmor_cli","text":"","title":"Deployment Steps for KubeArmor &amp; kArmor CLI"},{"location":"open-source/kubearmor-install/#1_download_and_install_karmor_cli","text":"curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin","title":"1. Download and install karmor CLI"},{"location":"open-source/kubearmor-install/#2_install_kubearmor","text":"karmor install It is assumed that the k8s cluster is already present/reachable and the user has rights to create service-accounts and cluster-role-bindings.","title":"2. Install KubeArmor"},{"location":"open-source/kubearmor-install/#3_deploying_sample_app_and_policies","text":"","title":"3. Deploying sample app and policies"},{"location":"open-source/kubearmor-install/#a_deploy_sample_multiubuntu_app","text":"kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/master/examples/multiubuntu/multiubuntu-deployment.yaml","title":"a. Deploy sample multiubuntu app"},{"location":"open-source/kubearmor-install/#b_deploy_sample_policies","text":"kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/master/examples/multiubuntu/security-policies/ksp-group-1-proc-path-block.yaml This sample policy blocks execution of sleep command in ubuntu-1 pods.","title":"b. Deploy sample policies"},{"location":"open-source/kubearmor-install/#c_simulate_policy_violation","text":"$ kubectl -n multiubuntu exec -it POD_NAME_FOR_UBUNTU_1 -- bash # sleep 1 ( Permission Denied ) Substitute POD_NAME_FOR_UBUNTU_1 with the actual pod name from kubectl get pods -n multiubuntu .","title":"c. Simulate policy violation"},{"location":"open-source/kubearmor-install/#4_getting_alertstelemetry_from_kubearmor","text":"","title":"4. Getting Alerts/Telemetry from KubeArmor"},{"location":"open-source/kubearmor-install/#a_enable_port-forwarding_for_kubearmor_relay","text":"kubectl port-forward -n kube-system svc/kubearmor 32767 :32767","title":"a. Enable port-forwarding for KubeArmor relay"},{"location":"open-source/kubearmor-install/#b_observing_logs_using_karmor_cli","text":"karmor log","title":"b. Observing logs using karmor cli"},{"location":"open-source/kubearmor-install/#k8s_platforms_tested","text":"Google Kubernetes Engine (GKE) Container Optimized OS (COS) GKE Ubuntu image Amazon Elastic Kubernetes Service (EKS) Self-managed (on-prem) k8s Local k8s engines (microk8s, k3s, minikube)","title":"K8s platforms tested"},{"location":"open-source/what-is-cilium/","text":"Cilium | eBPF-based Networking, Observability, and Security \u00b6 Cilium is an open source project to provide eBPF-based networking, security, and observability for cloud native environments such as Kubernetes clusters and other container orchestration platforms. We are focused on adding value to Cilium in the following areas by using: Extensible Identity solution based on SPIFFE standards Improving policy audit handling Improving policy telemetry and statistics collection to fit realistic scenarios Policy discovery tools.","title":"What is Cilium"},{"location":"open-source/what-is-cilium/#cilium_ebpf-based_networking_observability_and_security","text":"Cilium is an open source project to provide eBPF-based networking, security, and observability for cloud native environments such as Kubernetes clusters and other container orchestration platforms. We are focused on adding value to Cilium in the following areas by using: Extensible Identity solution based on SPIFFE standards Improving policy audit handling Improving policy telemetry and statistics collection to fit realistic scenarios Policy discovery tools.","title":"Cilium | eBPF-based Networking, Observability, and Security"},{"location":"open-source/what-is-kubearmor/","text":"KubeArmor | Cloud Native Runtime Security Enforcement System \u00b6 KubeArmor is a cloud-native runtime security enforcement system that restricts the behavior (such as process execution, file access, and networking operation) of containers and nodes at the system level.","title":"What is KubeArmor"},{"location":"open-source/what-is-kubearmor/#kubearmor_cloud_native_runtime_security_enforcement_system","text":"KubeArmor is a cloud-native runtime security enforcement system that restricts the behavior (such as process execution, file access, and networking operation) of containers and nodes at the system level.","title":"KubeArmor | Cloud Native Runtime Security Enforcement System"}]}